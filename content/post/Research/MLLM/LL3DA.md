---
title: LL3DA
date: 2024-06-27T11:30:03+00:00
tags:
  - 3DLLM
  - MLLM
categories:
  - MLLM
author: ZhaoYang
showToc: true
TocOpen: true
draft: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
---


#### abstract

大型多模态模型 (LMM) 的最新进展使得人机交互中的各种应用成为可能。然而，开发能够在复杂多样的 3D 环境中理解、推理和规划的 LMM 仍然是一个具有挑战性的课题，尤其是考虑到理解 3D 场景的置换不变点云 3D 表示的需求。现有的工作寻求多视图图像的帮助，并将 2D 特征投射到 3D 空间作为 3D 场景表示。然而，这会导致巨大的计算开销和性能下降。在本文中，我们介绍了 LL3DA，这是一种大型语言 3D 助手，它将点云作为直接输入并响应文本指令和视觉提示。这有助于 LMM 更好地理解人类互动，并进一步帮助消除混乱的 3D 场景中的歧义。实验表明，LL3DA 取得了显著的效果，在 3D 密集字幕和 3D 问答方面均超越了各种 3D 视觉语言模型。

#### introduction

大型语言模型 (LLM) 家族的近期激增 [13, 27, 41, 49, 58] 为以通用方式解决各种机器学习任务提供了巨大的机会 [16, 28, 32, 34]。在这次 LLM 狂欢节期间，研究人员也在寻求针对各种视觉语言任务的通用 LLM 解决方案 [16, 54, 59]。其中，基于 LLM 的 3D 场景理解是一个有价值的主题，将有利于自动驾驶 [8, 22] 和具身 AI 代理 [20, 47] 的发展。然而，考虑到 1) 3D 环境的多样性和复杂性以及 2) 理解稀疏 3D 点的需求，这也具有挑战性。先前的研究已经在解决各种 3D 视觉和语言任务方面取得了初步成功。主流研究构建了旨在解决某一特定下游任务的 3D 专家，包括 3D 问答 (3DQA) [2, 37]、3D 视觉基础 (3D-VG) [6, 52] 和 3D 密集字幕 (3D-DC) [9, 11]。同时，其他作品 [4, 12, 30, 65] 研究了不同 3D 视觉和语言任务与对象之间共享结构建模关系的相互促进。最近，研究人员还引入了用于通用 3D 理解的 LLM，其中 Point-Bind 和 Point-LLM [23, 54] 主要关注 3D 对象的理解。同时，3DLLM [26] 提出了一种 LLM 驱动的解决方案，它将多视图特征聚合为 3D 特征，展现出强大的能力，使机器能够理解各种 3D 对象和场景，并遵循人类产生的文本指令。

虽然这些方法在用自然语言理解 3D 世界方面取得了显著的成功，但也存在一定的局限性。
在有限的监督下，3D 专家很难扩大规模以获得更好的性能，而联合预训练仍然需要单独的主管来完成特定任务。提取多视图特征会导致巨大的计算开销，并且忽略了基本的几何属性。此外，纯文本通常会导致歧义，尤其是在混乱和复杂的 3D 环境中。
为了解决上述问题，我们提出了 LL3DA，这是一种大型语言 3D 助手，可以响应人类的文本和视觉交互，并在复杂的 3D 环境中理解、推理和规划（图 1）。我们采用多模态转换器，通过注意机制将来自文本指令、视觉提示和 3D 场景的信息聚合成固定长度的可学习查询标记。查询标记被投影并用作文本指令的前缀，作为预先训练和冻结的 LLM 的输入。这种设计不仅有助于解决置换不变的 3D 场景嵌入与 LLM 嵌入空间之间的矛盾，而且还提取了交互感知的 3D 场景嵌入以实现高效的指令跟踪。我们进行了广泛的实验，以探索 LLM 在复杂多样的 3D 环境中理解、推理和规划的能力。我们的模型在两个广泛使用的数据集上取得了最先进的结果，用于 3D 密集字幕 [1, 6] 和 3D 问答 [2]。此外，通过引入额外的视觉交互，我们的方法可以进一步消除模糊文本指令中的歧义。 
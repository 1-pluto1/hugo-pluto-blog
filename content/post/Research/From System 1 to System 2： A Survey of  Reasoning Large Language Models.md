---
title: "From System 1 to System 2: A Survey of  Reasoning Large Language Models"
date: 2025-03-16T11:30:03+00:00
tags:
  - LLM
  - ReasoningModel
  - Survey
categories:
  - AI
  - Research
author: ZhaoYang
showToc: true
TocOpen: true
draft: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
---

### abstract

实现人类水平的智能需要优化从快速、直觉的系统1到更缓慢、更审慎的系统2推理的过渡。系统1擅长快速、启发式的决策，而系统2则依赖于逻辑推理以做出更准确的判断并减少偏差。基础大语言模型（LLMs）在快速决策方面表现出色，但由于尚未完全具备真正系统2思维所特有的逐步分析能力，它们在复杂推理方面仍显不足。最近，如OpenAI的o1/o3和DeepSeek的R1等推理大语言模型在数学和编程等领域展示了专家级的表现，高度模仿了系统2的审慎推理，并展现了类人的认知能力。本文首先简要回顾了基础大语言模型的进展以及系统2技术的早期发展，探讨了它们的结合如何为推理大语言模型铺平了道路。接着，我们讨论了如何构建推理大语言模型，分析了其特征、实现高级推理的核心方法以及各类推理大语言模型的演变。此外，我们还提供了推理基准的概述，深入比较了代表性推理大语言模型的性能。最后，我们探讨了推进推理大语言模型发展的有前景方向，并维护了一个实时GitHub仓库以跟踪最新进展。我们希望本文能成为这一快速发展的领域中激发创新和推动进步的宝贵资源。

### INTRODUCTION

实现人类水平的智能需要优化从系统1到系统2推理的过渡[1]–[5]。双系统理论认为，人类认知通过两种模式运作：系统1，它是快速、自动和直觉的，能够以最小的努力做出快速决策；系统2，它是更慢、更具分析性和审慎的[6], [7]。尽管系统1在处理日常任务时效率很高，但它容易受到认知偏差的影响，特别是在复杂或不确定的情况下，从而导致判断错误。相比之下，系统2依赖于逻辑推理和系统性思考，从而做出更准确和理性的决策[8]–[11]。通过减少系统1的偏差，系统2提供了一种更精细的问题解决方法[12]–[15]。

  

基础大型语言模型（LLMs）的发展标志着人工智能（AI）领域的一个重要里程碑。诸如GPT-4o [16]和DeepSeekv3 [17]等模型在文本生成、语言翻译和各种感知任务中展示了令人印象深刻的能力 [18]–[28]。这些模型通过广泛的数据集训练和先进的算法，在理解和生成类人响应方面表现出色。然而，尽管它们取得了令人瞩目的成就，基础LLMs的运作方式类似于系统1的推理，依赖于快速、启发式驱动的决策。虽然它们在提供快速响应方面表现优异，但在需要深度逻辑分析和复杂推理任务精确性的场景中，它们往往表现不足。这一限制在涉及复杂问题解决、逻辑分析或细微理解的情境中尤为明显，这些模型尚未达到人类的认知能力。

  

相比之下，推理型大型语言模型（LLMs）代表了语言模型进化中的一个重要进步。(在本文中，“推理”指的是回答涉及复杂、多步骤过程的问题，并包含中间步骤。基础型LLMs：具备基本推理能力的LLMs，能够处理简单或单步任务。推理型LLMs：擅长复杂任务（如编程和数学证明）的LLMs，融入“思考”过程——这些任务是基础型LLMs难以胜任的)。像OpenAI的o1/o3 [29], [30]和DeepSeek的R1 [31]这样的模型，旨在模拟与系统2思维相关的更慢、更审慎的推理过程。与基础型LLMs不同，推理型LLMs配备了逐步处理信息的机制，使其能够做出更准确和理性的决策。这种从快速思维、直觉过程向更具系统性、推理驱动模型的转变，使推理型LLMs能够以专家级性能处理复杂任务，如高等数学[32]–[37]、逻辑推理[38]–[44]和多模态推理[45]–[47]，展现出类人的认知能力。因此，推理型LLMs越来越被认为能够实现深度逻辑思维，完成曾经被认为超出AI能力范围的任务。推理型LLMs的最新时间线如图1所示。

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=MzRiYTlhZDk1YmQ2MGI5ODZmODdmOTJkYTVjOTkzMDlfZlh4VmNUT3o3YnExVEZHT3kyd1dPOXB5SnZYUFdlWEhfVG9rZW46VVJ0bmJHTVJUb0o2b0Z4cU5KSWNsb1NDblhiXzE3NDg3NTgzNTY6MTc0ODc2MTk1Nl9WNA)

#### Structure of the Survey

本综述全面概述了推理型大型语言模型（LLMs）发展中的关键概念、方法和挑战。如图2所示，本综述的组织结构如下：

1. 第2节简要回顾了基础型LLMs的进展（第2.1节）以及关键系统2技术的早期发展，包括符号逻辑系统（第2.2节）、蒙特卡洛树搜索（MCTS）（第2.3节）和强化学习（RL）（第2.4节），重点介绍了它们的结合如何为推理型LLMs的发展铺平了道路。
    
2. 第3节介绍了推理型LLMs，并概述了它们的构建过程。具体而言，第3.1节从输出行为（第3.1.1节）和训练动态（第3.1.2节）两个角度展示了推理型LLMs的特征，强调了它们与基础型LLMs的区别。第3.2节确定了实现高级推理能力所需的核心方法，重点关注五个方面：结构搜索（第3.2.1节）、奖励建模（第3.2.2节）、自我改进（第3.2.3节）、宏观行动（第3.2.4节）。以及强化微调（第3.2.5节）。每一节都深入探讨了这些方法的具体特点，并介绍了每种方法的代表性推理型LLMs。第3.3节追溯了推理型LLMs的演化阶段。
    
3. 第4节评估了代表性的推理型LLMs。具体而言，第4.1节回顾了当前主流的推理基准，涵盖了各种任务类型的纯文本和多模态基准。第4.2节概述了当前的评估指标，而第4.3节基于这些基准分析和比较了主流推理型LLMs与其基础型对应模型的性能。
    
4. 第5节强调了现有推理型LLMs的局限性，并概述了这些模型未来发展的几个有前景的方向。
    
5. 最后，我们在第6节总结了本文，并提供了一个实时跟踪的GitHub仓库，以监控该领域的最新发展。
    

我们希望这篇综述能成为推动这一快速发展的领域创新和进步的有价值的资源。

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=ODUxMjNjYmRlYWY2MzYzMDE4ZmY4YzAyZTRjYzM4ZGVfalM5b0ZJZ2dQRlVDY3VPYlg4cnNHYVVtVXlVS2pYRnNfVG9rZW46SkFQNGJhc2xkb21VaDl4aG5RdWN1NjNtbmlxXzE3NDg3NTgzNTY6MTc0ODc2MTk1Nl9WNA)

#### Contribution of the Survey

最近，已经对特定技术方法进行了多次分析和复制[48]–[55]，但仍然缺乏系统性的分析和组织。研究[56]仅关注测试期间的慢思考方法。同时，研究[57]–[59]主要集中在训练或实现推理型LLMs，通常从强化学习（RL）的角度出发。我们的综述在以下方面与现有文献区分开来并做出了贡献：

1. 我们不仅关注单一技术方法，而是全面概述了推理型LLMs所涉及的关键概念、方法和挑战。
    
2. 我们总结了早期系统2的关键进展，以及它们如何为推理型LLMs铺平道路，特别是与基础型LLMs的结合——这是以往工作中经常被忽视的关键方面。
    
3. 我们对构建推理型LLMs所需的核心方法进行了更全面和包容的总结，包括但不限于强化学习（RL）。
    

  

### FOUNDATIONS OF REASONING LLMS

在本节中，我们简要概述了基础型LLMs的进展以及关键系统2技术的早期发展，重点介绍了与基础型LLMs结合后为推理型LLMs铺平道路的关键进展。这些进展包括符号逻辑系统、MCTS和RL。

#### Foundational LLMs

基础型LLMs的发展在2018-2019年引入了预训练的Transformer模型[18]后取得了显著进展，特别是通过BERT[19]和GPT[21]。这些模型利用大量文本语料库进行无监督预训练，然后针对特定任务进行微调。这种方法使它们能够在专门处理情感分析、实体识别和问答等任务之前，发展出广泛的语言理解能力。BERT的双向上下文处理提高了对单词的理解，而GPT的单向设计在文本生成方面表现出色。

  

2019年发布的GPT-2 [22]拥有15亿参数，标志着生成性能的显著飞跃，尽管它也引发了伦理担忧。GPT-3 [23]拥有1750亿参数，进一步展示了无监督预训练的强大能力，在少样本学习中表现出色，并在广泛的NLP任务中表现优异。在随后的几年中，像CLIP [60]和DALL-E [61]这样的多模态模型出现，整合了文本和视觉输入。这些模型实现了从文本生成图像等新任务，并增强了人机交互。

  

到2023-2024年，诸如GPT-4/4o [16], [62]、LLaMA [25]和LLaVA [27]等模型展示了在推理、上下文理解和多模态推理方面的先进能力，能够同时处理文本和图像 [63]–[65]。DeepSeek-V3 [17]采用了671B的专家混合架构 [66]–[68]，在关键基准测试中表现优于其他多个LLM，同时在效率和处理速度方面提供了显著改进。基础型LLM的演进彻底改变了人工智能，使其在语言理解、问题解决和人机协作方面实现了更复杂的应用。

  

总结：基础LLMs的发展经历了从BERT等预训练模型到GPT - 4等多模态模型的发展，在语言理解、文本生成和图像处理等方面取得了很大的进展。这一进步导致了人工智能的重大突破，提高了语言理解、问题解决和人机交互能力。基于深度学习的发展[ 18 ]，[ 69 ] - [ 83 ]，基础LLMs可以从大量的文本或多模态数据中学习广泛的世界知识和语义关系。这使得他们能够表现出诸如情境学习( ICL ) [ 84 ]，[ 85 ]，快速工程[ 86 ]，[ 87 ]和思维链( CoT )推理[ 2 ]等新兴能力，显著增强了他们的适应能力和创造性解决问题的能力。

  

尽管取得了这些进展，基础的大型语言模型（LLMs）仍然类似于系统1的推理方式，依赖于快速、启发式驱动的决策，缺乏系统2所特有的逐步分析能力。然而，它们的发展为未来的推理型LLMs奠定了坚实的基础——尤其是在与早期的系统2技术相结合时。这种结合为更通用、灵活且类似人类推理的模型铺平了道路。

#### Symbolic Logic Systems

符号逻辑系统标志着人工智能的最早阶段，利用规则和逻辑原则来表示知识并得出结论[88]，[89]。它们在结构化领域中特别有效，因为形式逻辑确保了精确性。Prolog是一种基于一阶逻辑的逻辑编程语言，允许用户定义事实、规则并通过查询进行推理。它在符号推理系统中至关重要，尤其是在自然语言处理（NLP）和专家系统中[90]–[92]。像Prolog这样的基于逻辑的系统使用命题逻辑和谓词逻辑进行形式推理[93]，[94]。从20世纪60年代到80年代初，这种方法主导了人工智能，例如IBM的LISP[95]用于符号计算，以及Resolution Theorem Provers[96]用于自动推理。在20世纪70年代，Marvin Minsky引入了框架（Frames），将知识组织成结构化框架，影响了专家系统和认知科学[97]。

  

概括：符号逻辑系统是早期人工智能发展的关键里程碑。基于形式逻辑，它们在定义明确的问题中表现出色，尤其是在结构化环境中。然而，它们也暴露了僵化、基于规则的系统的局限性。尽管存在这些限制，符号逻辑仍然是人工智能进步的基础。最近在推理大语言模型（LLMs）方面的进展，通过复杂的思维架构（称为宏动作框架，见第3.2.4节），极大地增强了模拟人类系统2认知过程的能力。通过将符号模板或规则与基础LLMs结合，宏动作显著提升了它们的推理能力。将宏动作整合到基础LLMs中，改变了它们处理复杂推理任务的能力，因为分层规划使模型能够在深入具体问题细节之前做出高层决策，这反映了符号逻辑的结构化方法。

#### Monte Carlo Tree Search

  

蒙特卡洛树搜索（MCTS）是一种基于模拟的搜索算法，用于决策和规划[98]。它通过四个步骤构建搜索树：**选择**，使用UCB1公式选择优先级最高的子节点：

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=MTc1MjZjOWFiYjAzNTJjZjcxYTllZjE0NzE2NjUwYzhfYnhXMTJHWTlKd2s5QUZ0ZGgxWGFqbWtVQnR5NHkxUHdfVG9rZW46RkJ3YWJEUE8wbzVIQWt4SUZ5OWNDemFUbmVlXzE3NDg3NTgzNTY6MTc0ODc2MTk1Nl9WNA)

其中wi是节点i的总奖励，ni是其访问次数，N是父节点的访问次数，c用于平衡探索与利用。**扩展**添加新节点，**模拟**执行随机推演以评估节点，**反向传播**更新节点统计信息。MCTS已广泛应用于优化策略任务，如围棋[99]和机器人路径规划，帮助机器人在动态环境中有效导航[100]。

**总结**：MCTS在推理大语言模型（LLMs）的发展中发挥了关键作用，尤其是在结构化搜索（第3.2.1节）中。通过模拟潜在的未来推理路径并反向传播估计的奖励，MCTS帮助基础LLMs高效识别最有前景的高奖励路径。这一过程类似于人类规划，即在采取行动之前考虑决策的未来后果。通过动态探索多种推理轨迹，MCTS使模型能够避免陷入次优路径，从而更容易在复杂的决策空间中导航。这种集成显著增强了LLMs处理复杂和动态推理问题的能力，例如需要长期规划或多步逻辑推理的任务。它使LLMs能够做出更具战略性和信息性的决策，从而提高了其在涉及细致推理和战略探索任务中的整体表现。

#### Reinforcement Learning

RL（强化学习）是一种机器学习方法，其中智能体通过与环境交互并根据反馈（奖励）学习决策，目标是最大化长期累积奖励 [101]。RL的早期突破，如Q-learning [102] 和DQNs（深度Q网络）[103]，通过使用深度神经网络（DNNs）处理复杂状态空间，彻底改变了这一领域 [104]。这些方法为将RL扩展到现实世界任务铺平了道路，而传统的表格方法在这些任务中表现不足。深度RL的出现标志着重大进展，它将深度学习与RL结合，能够处理高维输入，如图像和非结构化数据。深度RL的一个里程碑成就是AlphaGo，它通过自我对弈在复杂的围棋游戏中击败了世界冠军，展示了RL的潜力。这些成功案例突显了深度强化学习（Deep RL）在具有大规模连续动作空间和不确定性的环境中表现出色的能力。在此基础上，AlphaZero通过自我对弈、蒙特卡洛树搜索（MCTS）和深度神经网络（DNNs）掌握了多种棋盘游戏——国际象棋、围棋和将棋，进一步推进了这一方法 [106]。AlphaZero能够完全从零开始学习，无需先验的人类知识，展示了RL在需要长期策略和规划的环境中的强大能力。

  

AlphaStar进一步扩展了深度RL的边界，在即时战略游戏《星际争霸II》中表现出色。与棋盘游戏不同，《星际争霸II》提供了动态的、部分可观测的环境，并要求多步骤的实时决策 [107]。AlphaStar在这一领域的成功展示了深度RL在适应复杂决策场景方面的能力，这些场景既需要战略规划，也需要战术执行。

RL和深度RL的这些进展极大地扩展了AI的潜力，从定义明确、静态的环境过渡到需要持续学习和适应的动态复杂环境。

  

**总结**：深度RL在解决复杂决策任务方面被证明非常有效。AlphaGo通过自我对弈学习策略并击败了围棋世界冠军，成为这一领域的典范。这种自我对弈的概念为推理大语言模型（LLMs）中的自我改进技术（第3.2.3节）奠定了基础，两者都依赖于持续的反馈和调整来优化策略。

  

在强化学习（RL）中，奖励塑造（reward shaping）至关重要，尤其是在多步推理任务中[108]。通过调整奖励信号以在中间步骤提供更细粒度的反馈，它帮助智能体在复杂的决策路径中导航。这一概念启发了推理大语言模型（LLMs）中奖励建模（Reward Modeling，第3.2.2节）的发展，特别是过程奖励模型（process reward model）。该模型提供逐步监督，以识别和纠正推理过程中的错误。通过模仿人类推理，过程奖励模型确保了更稳健和可解释的结果，尤其是在数学问题求解和代码生成等需要逐步评估的任务中。

此外，RL本身也是推理LLMs的强大工具（第3.2.5节）。通过奖励机制，RL引导基础LLMs找到最优解，特别是在动态推理问题中。其简单性和高效性使得RL在训练和优化推理LLMs中不可或缺，从而提升了AI模型的智能和自我进化能力。RL的整合推动了推理LLMs的重大进展，如DeepSeek-R1[31]所展示的，提供了更灵活和高效的解决方案。

### BLUEPRINTING REASONING LLMS

在本节中，我们首先从输出行为和训练动态两个角度分析推理大语言模型（LLMs）的特征。然后，我们详细概述了实现其高级推理能力的核心方法。最后，我们总结了推理LLMs的演变过程。传统推理模型与推理LLMs的全面对比见图3。

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=OWY5Y2E0MTUxODcyZTNmNzNkYTVkZTFmMmEzM2JjMzVfWmVVNmxER3dJN00xbktZYlBPUXRvT0RmSldTeGJQWWlfVG9rZW46TUZ1eGJzdVRIb2t2aHJ4NlFFTGNRVUhWbkxMXzE3NDg3NTgzNTY6MTc0ODc2MTk1Nl9WNA)

#### Analysis of the Features of Reasoning LLMs

##### Output Behaviour Perspective

**探索与规划结构**：最近的实证研究表明，推理大语言模型（LLMs）在其输出结构中表现出强烈的探索性行为，尤其是与主要依赖传统思维链（CoT）推理方法的模型（如WizardMath [109]和DeepSeekMath [110]）相比。这种探索性行为体现在新假设的提出和替代解决路径的追求上。[49]的研究表明，慢思考模型在预测后续标记时，会进行一种潜在的生成过程，这一现象尤为明显。[31]的研究也支持了这一观点，观察到类似行为在RL规模训练中自然出现。此外，Quiet-STaR框架 [111] 引入了一个专注于下一标记预测的辅助预训练阶段，强调了在内容生成之前内部深思和探索机制的关键作用。这些发现共同强调了高级LLMs中推理过程的复杂性和动态性，突出了其操作框架内探索与结构化推理之间的相互作用。

**验证与检查结构**：对OpenAI的o1 [29]和o3 [30]模型的分析表明，它们的推理框架结合了用于长期战略规划的宏观动作和包括“等待”、“稍等”、“或者”、“让我们暂停”等微观动作。这些微观动作促进了细致的验证和迭代检查过程，确保了任务执行的精确性。这种双层方法突显了模型在平衡总体目标与细节导向操作方面的能力，从而增强了其整体功能和可靠性。为了模拟这一特性，Marco-o1 [112]在构建长思维链（Long-CoT）的MCTS过程中，为每个树节点分配了“等待！可能我犯了一些错误！我需要从头重新思考”的状态，从而促进了Long-CoT的反思性质。Huatuo-o1 [113]采用多代理框架来解决验证过程中生成错误思维链的问题，通过引入具有“回溯”和“修正”功能的提示，实现了修正过程。

**更长的推理长度与时间**：最近的研究[49]–[52]、[114]表明，推理型大语言模型（LLMs）在处理编码和数学中的复杂问题时，生成的输出通常超过2000个词元。然而，这种较长的输出有时可能导致“过度思考”，即模型在问题上花费过多时间，却未必能提升解决方案的质量。研究[49]指出，尽管自回归生成和经典链式思维（Classic CoT）能有效解决较简单的问题，但在处理更复杂的任务时表现欠佳。研究[115]、[116]显示，在多模态领域中，许多问题需要细致的观察、比较和深思熟虑。此外，Search-o1[117]提出，慢思考机制在需要外部知识或可能出现知识冲突的领域中尤为有益。在医疗场景中，复杂问题（如需要测试时扩展技术的问题）展现了显著的改进[52]。

**过度谨慎与简单问题陷阱**：目前，推理型大语言模型（LLMs）在多个领域展现了强大的性能，例如竞赛级数学[31]、[54]、[118]、[119]、复杂编程[120]、医疗问答[52]、[113]以及多语言翻译[112]、[121]。这些场景要求模型对问题进行细粒度分析，并根据给定条件进行严谨的逻辑推理。有趣的是，即使对于像“2+3=？”这样简单的问题，推理型LLMs也可能表现出过度自信或不确定性。最近的研究[122]指出，类似o1的模型在处理较简单的数学问题时，往往会生成多轮解决方案，探索不必要的路径。这种行为与对简单问题缺乏多样化探索形成了鲜明对比，表明模型的推理过程可能存在效率低下的问题。

##### Training Dynamic Perspective

**惊人的数据效率**：与传统方法专注于扩展难度均匀分布的指令集不同，研究[52]、[54]表明，构建以困难样本为重点的慢思考链式思维（Slow-thinking CoT）数据集，能够在医学和数学等领域实现更好的泛化效果。这种方法与传统收集多样化且均匀分布的指令数据集的实践有所不同。

**稀疏训练方法**：与传统观念相反，开发高效的推理型大语言模型（LLMs）并不需要大规模数据集或密集的奖励信号。例如，STILL2[51]仅使用5,000个蒸馏样本就展示了令人印象深刻的表现，而Sky-T1[119]仅用17,000个LongCoT样本就实现了与QwQ[118]相当的性能。同样，RedStar[54]仅用4,000个核心LongCoT样本就在文本和多模态任务中取得了卓越的成果。与简单的链式思维（CoT）相比，慢思考监督微调（SFT）数据展现了显著的样本效率，通常只需1/100的样本量即可达到可比的结果。此外，研究[123]强调了在线强化学习（RL）扩展算法的巨大训练潜力，表明非密集的RL监督甚至基于规则的奖励结构足以实现高性能。

**参数特性**：通过LongCoT方法训练慢思考的大语言模型（LLMs）会导致不同层之间的梯度范数相对均匀。相比之下，以简化CoT方法为代表的快思考则在早期层产生较大的梯度幅度，并且各层之间的梯度范数存在显著差异。实证研究表明，更大的模型（尤其是参数超过300亿的模型）由于其更强的复杂推理能力，更适合用于推理型LLMs的训练。此外，RedStar[54]的实验表明，数据扩展的效果因模型规模而异，扩展效应在更大模型中更为显著和有效。这一发现得到了Deepseek-R1研究[31]的支持，该研究表明，一个6700亿参数的模型能够实现与o1基准接近的性能指标，凸显了更大规模架构在高级推理任务中的可扩展性优势。

#### Core Method

在本节中，我们概述了推动推理型大语言模型（LLMs）高级推理能力的核心方法，如图4所示。这些方法包括**结构搜索（Structure Search）**、**奖励建模（Reward Modeling）**、**自我改进（Self Improvement）**、**宏观动作（Macro Action**）以及**强化微调（Reinforcement Fine-Tuning**）。我们还为每种方法列举了具有代表性的推理型LLMs。

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDQxODlmODdmODAzMWM5ZmZmMjBjZjY5Y2MxNGMxNmRfcFZZOXgxNUxRd2NYY0Z2NmlqY09CUlZCYmk2Q1pXRDJfVG9rZW46VGY1Y2I4RGZhbzhURXB4U0k0bWNoRHZWblJkXzE3NDg3NTgzNTY6MTc0ODc2MTk1Nl9WNA)

##### Structure Search

---
title: "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought"
date: 2025-02-26T11:30:03+00:00
tags:
  - LLM
  - NLP
  - TODO
categories:
  - AI
  - Research
author: ZhaoYang
showToc: true
TocOpen: true
draft: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
---
_**长文预警！！！50页左右**_

单位： SynthLabs.ai, Stanford University, UC Berkeley

代码：

基座模型：

原文地址：https://arxiv.org/abs/2501.04682

数学领域的benchmark：

- Hendrycks MATH
    
- HARP
    
- Omni-MATH
    

### Introduction

**我们提出了一种新颖的框架——元思维链（Meta Chain-of-Thought, Meta-CoT），该框架通过显式建模生成特定思维链（CoT）所需的底层推理，扩展了传统的思维链（CoT）。我们展示了来自最先进模型的经验证据，这些模型表现出与上下文搜索一致的行为，并探索了通过过程监督、合成数据生成和搜索算法生成Meta-CoT的方法。最后，我们概述了一个具体的训练管道，用于训练模型生成Meta-CoT，结合了指令调优与线性化搜索轨迹以及强化学习的后训练。我们还讨论了开放的研究问题，包括扩展法则、验证器的作用以及发现新型推理算法的潜力。这项工作为在大型语言模型（LLMs）中实现Meta-CoT提供了理论和实践路线图，为人工智能实现更强大、更类人的推理铺平了道路。**

  

### Meta Chain-Of-Thought

#### Deriving The Meta-CoT Process

Reasoning data present in pre-training corpuses does not represent the true data generation process, especially for complex problems, which is a product of extensive latent reasoning. Moreover, this process generally does not occur in a left-to-right, auto-regressive, fashion.

  

预训练语料库中的推理数据并不代表真实的数据生成过程，尤其是对于复杂问题而言，这些问题是广泛潜在推理的产物。此外，这一过程通常不会以从左到右、自回归的方式发生。

  

Essentially, to start generating the solution requires that we already know the full approach.

本质上，要开始生成解决方案，需要我们已经了解完整的解决思路。

  

We can formalize this argument through the interpretation of reasoning as a latent variable process (Phan et al., 2023). In particular, classical CoT can be viewed as:\

经典COT:

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=OTE3ODlhODI0ZmRiM2JhZjg3ZDJkYjUyZDEzNjdhZjFfRlowRTk4YlZNZUYyYlhlS3BzZEhFZVNvMWhvZG5jUXVfVG9rZW46UlVmZGJreUc0b2tKSmV4aDk1MmNHaU9tblNiXzE3NDg3NTkzMDE6MTc0ODc2MjkwMV9WNA)

MetaCOT:

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=NDE3MWY3NDRjOGYzM2U1MDQzYjBlY2E5YWM3ODRlMTNfRFU5d2tvUlhvOEY2WXF6UFRIbGV6N0QxTHlpQVFYOWdfVG9rZW46SHJtVmI5WHFCbzdWQzV4clgyZmNKSUNFbkRqXzE3NDg3NTkzMDE6MTc0ODc2MjkwMV9WNA)

#### Why Does (Classical) CoT Fail?

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGE0NTE3ZDE1NDdjMjU1ZWQxNTFmODdjODZlYzU3Nzdfd2FxZUxxdFRDcmtyVHMwMXEzTlI2VGNmSmp3ZUhzcmhfVG9rZW46WGFDU2JQMXpPb1ZCaEF4cVFabWNqd1FpblFmXzE3NDg3NTkzMDE6MTc0ODc2MjkwMV9WNA)

  

1. On level 1 problems the o1 series generates a comparable number of tokens to human-written solutions. These are the types of problems where the training solutions likely match the true data generation process and each individual logical step can be internalized in a constant-depth transformer.
    
2. 2. At higher difficulty, the o1 series of models generates significantly more tokens per problem and also widens the performance gap over the classical reasoning models. In fact the gap between the inference compute used by the o1 model and prior series of models seems to scale with the complexity of the problems. We hypothesize that in those more challenging problems the solutions do NOT in fact represent the true data generative process, which is instead better approximated by the more extensive Meta-CoT generated by the o1 family of models.
    

  

1. 在难度级别1的问题上，o1系列生成的标记数量与人类编写的解决方案相当。这类问题的训练解决方案可能与真实的数据生成过程相匹配，并且每个独立的逻辑步骤可以在恒定深度的Transformer中被内化。
    
2. 在更高难度的问题上，o1系列模型每个问题生成的标记数量显著增加，并且与经典推理模型的性能差距也进一步扩大。事实上，o1模型与之前系列模型在推理计算上的差距似乎随着问题的复杂性而扩大。我们假设，在这些更具挑战性的问题中，解决方案实际上并不代表真实的数据生成过程，而o1系列模型生成的更广泛的Meta-CoT更好地近似了这一过程。
    

### Towards Deliberate Reasoning With Language Models - Search

该论文认为：大型语言模型（LLMs）在高级推理任务上表现不佳，原因是训练数据未能充分代表真实的数据生成过程。

基于以上观点，提出了以下问题：what does the true data generating process look like?真正的数据生成过程是什么样子的？

1. First, we argue that for many advanced reasoning or goal-oriented problems there exist meaningful gaps between the complexity of generation and verification. This is of course one of the fundamental open problems of theoretical computer science and any attempt to prove this is significantly beyond the scope of the current writing, but we will review what we believe to be compelling empirical evidence from the literature.
    
2. Second, assuming a non-trivial generator-verifier gap, we argue that the solutions to challenging problems present in text corpora are the outcomes of an extended search process, which itself is not represented in the data.
    
3. 首先，我们认为对于许多高级推理或目标导向的问题，生成和验证的复杂性之间存在有意义的差距。这当然是理论计算机科学中的一个基本开放问题，任何试图证明这一点的尝试都远远超出了当前写作的范围，但我们将回顾我们认为是来自文献的有力经验证据。
    
4. 其次，假设存在一个非平凡的生成器-验证器差距，我们认为文本语料库中存在的挑战性问题的解决方案是扩展搜索过程的结果，而这一过程本身并未在数据中体现。
    

#### Inference-Time Compute: Search

3.1 推理时计算：搜索

上述第一点（生成-验证差距）最近在“部署推理时计算”的框架下成为了一个热门的研究和讨论方向，我们在第一个实验中对此进行了探索。我们从一个LLaMA 3.1 8B基础模型（Dubey等人，2024）开始，并在Numina MATH数据集（Li等人，2024）上进行了广泛的监督微调。结果见图2，数据集详细信息见第8.1节。对于每个中间检查点，我们在Hendrycks MATH（Hendrycks等人，2021）500题评估数据集（Lightman等人，2023）上评估性能。根据结果，我们在此提出几点观察：

  

1. 我们在中间检查点上评估了pass@k（即使用了一个“神谕验证器”），并观察到随着k的增加，性能显著提升。尽管零样本性能在贪婪解码下从大约20%提升到40%（见图2左侧的基础过滤器），但即使在pass@4上，第一个模型检查点也超过了这些结果（见图2右侧）。此外，8B模型最终检查点的pass@64达到了接近85%的准确率，超过了许多当前前沿模型的零样本性能。
    
2. 我们还评估了在k=8和k=64时的多数投票性能。随着训练量和样本量的增加，性能持续提升，maj@64在没有真实验证器的情况下，仅用15%的训练计算量就超过了贪婪模型的性能。
    

  

这些结果表明，即使我们通过在越来越多的SFT数据上进行微调直接优化答案生成能力，验证器-生成器之间的差距仍然存在，这从pass@k和多数投票设置中的性能提升中得到了证明。近期的文献也观察到了在后训练采样中的类似结果（Lightman等人，2023；Brown等人，2024；Snell等人，2024）。然而，这些研究大多没有系统地评估训练数据量、计算量和模型大小变化的影响，我们认为这是一个值得进一步实证研究的方向。这些问题很重要，因为从额外推理中获得的收益可能会在更大规模的训练中消失，即模型可能能够完全内化推理过程。这在高级模型和更简单的基准测试（如GSM8k）中似乎确实如此（Cobbe等人，2021）。尽管我们在实验中观察到了相反的结果，但我们承认这些结果只是初步研究的结果，还需要进一步的工作。然而，我们将在第6节从理论上论证，在具有足够高认识不确定性的领域中，持续的搜索差距仍然存在。

除了这一点之外，问题仍然存在：在没有神谕验证器或环境反馈的情况下，是否能够有效地实现通过增加推理获得的改进。从理论上讲，可以在增加推理预算的情况下生成正确的解决方案，但我们可能无法有效地验证它们，因为验证的复杂性可能与生成的复杂性一样高，甚至更高。我们将在接下来的部分讨论这个问题。

#### Inference-Time Compute: Verification

许多研究专注于训练验证器模型，这些模型明确评估推理步骤和解决方案的正确性。验证器可以通过显式的二元分类进行训练（Cobbe等人，2021；Lightman等人，2023；Snell等人，2024；Anonymous，2024；Setlur等人，2024b），或者通过自然语言直接建模评估，利用大型语言模型（LLM）作为法官的先验知识（Zhang等人，2024a；Mahan等人，2024）。这些方法的统一表述是模型 \( v_{\theta} \)，它评估一个推理过程 \( v_{\theta}(q, S) \rightarrow [0, 1] \)。在这个框架下，可以从固定的生成器 \( \pi_{\theta}(\cdot|q) \) 中生成 \( K \) 个候选解决方案 \( (S_1, \ldots, S_K) \)，并根据它们的评估分数进行排序。

\[ S^* = \arg\max\{v_{\theta}(q, S_1), \ldots, v_{\theta}(q, S_K)\} \]

关于实证结果，我们建议读者参考图3（来源：Zhang等人，2024a），该图评估了多个验证器模型 \( v_{\theta} \)。无论验证器的效率如何，通过额外的在线采样，性能都有显著提升。此外，使用明确训练的验证器模型优于简单的推理计算扩展策略，例如自一致性或多数投票。

关于使用固定生成模型（策略）的问题仍然存在：该模型是否训练不足？如果进一步训练，其零样本性能是否能够提升到不再需要额外在线搜索也能获得有意义改进的程度？我们将在第3.4节中讨论这一问题。

#### From Best-of-N To General Search

到目前为止，我们通过经验探索了最佳N方法，即独立生成多个完整解决方案，并根据分数选择最有前景的一个。然而，这种方法效率低下，因为它需要探索完整的解决方案路径，即使早期出现错误，并且可能会重复采样相同的正确步骤。相反，我们可以将推理建模为马尔可夫决策过程（MDP），定义为元组M（S，A，P，R，），其中：

• S：状态集，其中每个状态S ∈ S，由提示和迄今为止的生成组成，即St（q，s1，...，st）。

• A：动作集，其中每个动作a ∈ A将表示为下一个推理步骤at1 st1。

• P（s′ s，a）：转移概率函数，表示在状态s中采取动作a时转移到状态s′的概率。为简单起见，我们主要考虑确定性转移函数P（st1，（q，s1，...，st））→（q，s1，...，st，st1），该函数将下一个推理步骤附加到上下文中。通常，环境动态可能更复杂。例如，具有工具访问权限的模型必须调用实际工具并在上下文中接收环境反馈，甚至修改其环境，如SWE和Web代理的情况。

• R（s，a）：奖励函数，为在状态s中采取动作a提供标量奖励。我们将假设中间奖励为零，最终奖励为1表示正确解决方案，否则为零，尽管在存在良好的过程奖励模型时这不是严格必要的（Setlur等，2024c）。

• r∈ [0，1]：折扣因子，平衡进一步计算和奖励之间的权衡。

  

我们将生成推理步骤的LLM称为策略st1 ∼（St）。此外，我们将从s0 q开始的解决方案称为一个情节或轨迹。我们还将使用符号zt表示Meta-CoT中的单个推理步骤，并相应地表示Zt（q，z1，...，zt）。

在上一节中，我们考虑了生成和排名完整解决方案，这可能效率低下。我们可以扩展上一节中的解决方案验证器的概念，以估计特定中间状态将导致解决方案的概率：v（q，St）→ [0，1]。这些模型已更广泛地称为过程奖励模型（PRMs）（Lightman等，2023）。如果我们能够访问这样的模型，我们可以通过以下步骤提高搜索过程的效率：

1. 终止未取得进展或在达到最终答案之前不正确的解决方案尝试。
    
2. 将代理重置为任何先前访问过的、具有高成功可能性的中间状态。
    

  

  

理论上，树搜索并不会在并行采样的基础上带来根本性的能力转变，然而，正如Yao等人（2023）所展示的那样，它可能会显著提高效率。特别是，图5（源自Yao等人（2023））显示，在一个玩具推理问题（24点游戏）上，使用树结构搜索方法相比并行采样，在推理预算方面效率提高了近4倍。尽管这些早期研究主要关注简单推理任务上的零样本（或接近零样本）性能，但值得注意的是，树搜索方法已经成功地扩展到许多现实世界的智能体应用中。

#### Is Search (Inference Time Compute) A Fundamental Capability Shift?

作者质疑通过搜索获得的推理性能提升都可以通过增加模型参数来获得

### Towards Meta-CoT Reasoning

在之前的章节中，我们：介绍了元思维链（Meta-CoT）的概念，并论证了其在高级推理中的必要性；讨论了生成器-验证器差距作为一个根本性限制；提出了搜索作为元思维链的基本构建模块；并探讨了整合生成器、验证器和搜索组件的实用性。然而，问题仍然在于如何将这些组件整合到一个模型中，以实现元思维链或“系统2”推理。我们首先需要回答的问题是：为什么我们实际上需要将深思熟虑的推理内部化到一个单一模型中？我们提出了两个主要原因：

1. **效率**：通过在自回归模型的上下文中整合搜索，探索可以高效进行，因为模型可以访问上下文中所有先前访问过的节点。与自然语言推理的独特情况不同，许多分支可能包含语义相似的内容，这与其他领域（例如棋盘游戏）形成对比，从而推动了对效率提升的需求。事实上，即使是高级推理模型也会执行许多语义相同的重复推理步骤，正如我们在图14和图15中所展示的那样。
    
2. **超级智能**：如果自回归模型能够学会在上下文中实现搜索算法，那么额外的强化学习训练可能使模型发现新的推理方法。本质上，我们提出，训练一个能够内部化系统2推理（例如元思维链）和搜索的模型，是对算法而非特定输出的优化，这可能会产生新的问题解决模式。这将使模型能够解决在符号化树搜索方法下无法解决的某些类别的问题，正如我们在第3.3节和第3.4节中概述的那样。在本节的剩余部分，我们将探讨如何训练一个模型以内部化这样的推理系统。
    

  

#### Bootstrapping Meta-CoT

##### Self-Taught Reasoner

STaR方法引入了一种迭代自举方法，旨在提升大语言模型（LLMs）的推理能力（Zelikman等，2022）。STaR专注于训练模型生成并优化推理过程，特别是针对需要复杂推理的任务，采用基于强化学习的方式。在此框架中，我们假设可以访问一个数据集D = {q(i), a(i)}N i=1，其中包含需要推理的问题q及其对应的答案a。需要注意的是，我们并不需要这些问题的真实推理过程。我们首先通过提示模型生成推理过程S(i) = s(i)1, ..., s(i)Ni和最终答案a(i)，即(a(i), S(i)) ∼ (a, S|q(i))。随后，我们对生成的数据进行过滤，仅保留那些能够得出正确答案的推理过程（即a(i) = a(i)），从而构建一个包含问题、（自举生成的）推理过程和答案的数据集DSTaR = {q(i), S(i), a(i)}N i=1。DSTaR随后被用于以标准的监督微调目标训练模型：LSTaR(θ) = −E(q,S,a)∼DSTaR [−log pθ(a, S|q)]。（2）上述过程会重复多次迭代。STaR的核心思想是通过采样和验证生成一个包含合成推理过程的训练数据集。我们将在下文将这一思想扩展到Meta-CoT的概念中。

##### Meta-STaR

我们可以将上述思想直接推广到Meta-CoT。考虑一个基础策略与某种针对中间步骤的通用搜索程序相结合。给定一个问题q，我们重复执行搜索程序以生成搜索轨迹z1, . . . , zK，直到找到最终解(s1, . . . , sn)。如果我们能够验证最终生成的解v(S) → {0, 1}，例如通过使用形式化和验证方法（如AlphaProof1）或其他结果验证方法，那么我们可以将类似的方法应用于STaR。例如，我们可以构建一个数据集DSTaR {q(i), Z(i), S(i)}N i1，并使用与之前类似的训练目标：LMeta−STaR() −E(q,Z ,S)∼DSTaR [ − log (S, Zq) ] 。（3）本质上，我们可以使用基础策略和搜索程序生成合成搜索数据，然后通过Meta-CoT概念训练模型以在上下文中实现这些操作。我们实际上提出将第3节中描述的搜索方法线性化，并教导自回归模型按顺序运行它们。到目前为止，我们故意对这些搜索程序和数据集的具体形式保持模糊。接下来，我们将从文献中提供实际方法的示例和概念验证，以及现实训练数据的合成示例。

#### Empirical Examples Of Internalizing Search


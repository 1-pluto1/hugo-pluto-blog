---
title: "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models"
date: 2025-03-01T11:30:03+00:00
tags:
  - LLM
  - NLP
  - Survey
categories:
  - AI
  - Research
author: ZhaoYang
showToc: true
TocOpen: true
draft: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
---

### Abstract

语言长期以来被视为人类推理的重要工具。大型语言模型（LLMs）的突破引发了利用这些模型解决复杂推理任务的重大研究兴趣。研究人员通过引入“思维”概念——代表推理过程中中间步骤的令牌序列——超越了简单的自回归令牌生成。这一创新范式使LLMs能够模仿复杂的推理过程，如树搜索和反思性思维。最近，学习推理的新兴趋势应用强化学习（RL）来训练LLMs掌握推理过程。这种方法通过试错搜索算法自动生成高质量的推理轨迹，通过提供更多的训练数据显著扩展了LLMs的推理能力。此外，最近的研究表明，鼓励LLMs在测试时推理过程中使用更多的令牌可以进一步提高推理准确性。因此，训练时和测试时的扩展相结合展示了一个新的研究前沿——通向大型推理模型的路径。OpenAI的o1系列的引入标志着这一研究方向的一个重要里程碑。在本调查中，我们全面回顾了LLM推理的最新进展。我们首先介绍了LLMs的基础背景，然后探讨了推动大型推理模型发展的关键技术组件，重点关注自动化数据构建、学习推理技术和测试时扩展。我们还分析了构建大型推理模型的流行开源项目，并以开放挑战和未来研究方向作为结论。

### Introduction

在这篇调查中，我们对近期在大型推理模型发展过程中的研究努力进行了全面回顾。第2节简要介绍了LLM推理的背景。接下来的三个部分深入探讨了推动大型推理模型发展的关键技术组成部分。具体而言，第3节聚焦于训练数据的构建，强调了从人工标注向LLM驱动的自动化搜索的转变。第4节回顾了强化学习方法，这些方法在增加训练计算量的情况下对扩展LLM推理能力至关重要，而第5节讨论了测试时的扩展，特别强调了PRM引导的搜索。第6节分析了OpenAI的o1系列及其他开源项目的发展，探索了通向大型推理模型的路径。第7节总结了额外的测试时增强技术，第8节回顾了推理基准测试。最后，我们在调查的结尾讨论了开放问题和未来的研究方向。

### Background

#### Pre-trianing

在训练大型语言模型（LLMs）的基础阶段，有效的预训练对于发展推理能力至关重要。在讨论LLMs的推理预训练之前，我们首先概述了一般LLM预训练的基本过程。通过预训练，LLMs不仅获得了核心的语言知识，还获得了多样化的世界知识，为高级能力的出现和有效的价值对齐奠定了坚实的基础[191]。通常，LLM预训练依赖于高质量的文本语料库[35, 168]，包括大量的网络内容、书籍、代码和其他类型的数据。利用这些丰富的文本语料库，LLMs基于Transformer架构进行训练，并通过下一个词预测任务进行训练。预训练后，LLMs通常表现出卓越的上下文学习能力[14]，使它们能够生成连贯的文本，并利用其庞大的知识库准确回答各种问题。值得注意的是，预训练阶段在培养LLMs的推理能力方面起着关键作用。例如，研究[160]表明，富含代码和数学内容的数据集是发展强大推理能力的关键基础。基于这一观察，新开发的LLMs[1]开始引入精心设计的合成数据，以增强LLMs的推理能力。在预训练过程中，一个关键的挑战在于平衡代码和数学数据与一般文本语料库的比例，以保持强大的通用语言能力，同时释放LLMs的推理潜力。

#### Fine-tuning

尽管预训练使大型语言模型（LLMs）能够通过上下文学习展示推理能力，但微调技术被广泛用于实现LLMs的零样本和改进的推理能力。在此，我们首先概述了基本的微调过程，然后探讨其在增强推理能力方面的潜力。如[104]所述，在预训练阶段之后，LLMs进入监督微调阶段（SFT），也称为指令微调阶段。这一阶段的主要目标是优化模型的输出风格，确保其响应与人类需求和实际应用相一致。这是通过使用反映广泛日常人类互动的多样化指令数据集进行训练来实现的，这些数据集通常通过广泛且精心策划的手动注释和优化创建[195]。随着ChatGPT的出现，生成多样化指令数据集的新方法应运而生。这些方法包括直接从强大的LLMs中提取数据[153, 167]以及从现有语料库中大规模构建数据集的自动化方法[158, 32]。使用这些精心设计的指令微调数据集，微调过程持续使用与预训练相似的下一个词预测目标。然而，与预训练不同的是，微调专门计算答案的损失，而通常忽略问题的损失。此外，结合包含链式思维（CoT）[160]推理和数学问题解决示例的数据集已被证明能显著增强LLMs的推理能力，这使其成为一个活跃的研究领域。遵循一般实践，当前大多数方法利用从先进的大型推理模型中提取的数据，然后通过微调来增强LLMs的推理能力，从而获得最终的大型推理模型。

#### Alignment

仅仅依赖从先进的大型推理模型中直接提取数据，限制了新的大型语言模型（LLM）的潜力。一种更有前景的方法是使用强化学习进行数据构建和模型训练，这与一般LLM训练中的最终对齐阶段精确对应。在LLM的一般训练中，对齐阶段通常涉及诸如基于人类反馈的强化学习（RLHF）[104]等方法，以引导模型生成符合有用、无害和诚实标准的内容。这一阶段的目标是增强LLM在现实中的安全性和可控性。与之前的监督微调（SFT）阶段相比，这一阶段通常包含大量精心策划、手动标注的排序数据，以准确反映人类偏好[35, 168]。这些数据不仅包括正确的示范，还包括应避免的不良案例。标准的RLHF通常涉及一个SFT模型、一个奖励模型和一个对齐模型，这些模型通过如近端策略优化（PPO）[121]等方法进行迭代优化。由于标准RLHF对数据和训练成本的高要求，诸如直接偏好优化（DPO）[112]等方法被提出，以减少对显式奖励模型的依赖。在DPO中，偏好损失被定义为策略的函数，以直接指导模型优化。鉴于推理问题的多步骤性和复杂性，基于对齐的后训练已成为激发LLM推理能力的最终且最关键的一步。通过仔细分解推理过程并逐步将信号反馈给模型，基于强化学习和偏好学习的各种自训练方法[45, 64, 183]已取得了显著成功。

#### Prompting LLMs for Advanced Reasoning

类人推理是大型语言模型（LLMs）在具备足够大规模参数时涌现出的最重要能力之一 [157]。尽管零样本推理在某些任务中可能仍然不可靠，但研究人员已经发现了多种提示技术来增强这些能力。这些技术可以大致分为三种主要方法：逐步推理、多路径探索和基于分解的方法。

  

逐步推理方法以“思维链提示”（Chain-of-Thought prompting）为代表 [160]，研究表明，明确展示中间推理步骤能显著提高问题解决能力。即使是简单的提示，如“让我们一步一步思考”，也能有效引导推理过程 [62]。这一方法通过“自我一致性”（Self-Consistency）[153] 和“自动思维链”（Auto-CoT）[189] 得到了进一步优化，前者通过生成多条推理路径以得出更可靠的结论，后者则自动化生成有效的推理链。

多路径探索方法超越了线性推理，同时考虑多条潜在解决路径。“思维树”（Tree of Thoughts）[172] 将替代推理路径组织成树状结构，从而系统性地探索不同的解决策略。“思维图”（Graph of Thoughts）[11] 进一步将其推广为图结构，允许更灵活的推理模式和回溯能力。ReAct [173] 通过将推理与行动步骤交织，丰富了这一范式，实现了与外部环境的更动态交互。

对于复杂问题，基于分解的方法被证明特别有效。“从少到多提示”（Least-to-Most Prompting）[196] 和“思维算法”（Algorithm of Thoughts）[122] 系统地将复杂问题分解为可管理的部分，而“计划与解决”（Plan-and-Solve）[147] 则为处理这些子问题提供了战略指导。这些方法在处理需要多步骤或多层次分析的任务时尤其有价值。

通过结构化提示策略增强的这些广泛推理能力，已被证明在需要仔细分析和系统思维的任务中特别有效，使LLMs能够完成各种复杂的社会科学相关任务。这些方法的成功表明，尽管LLMs具备固有的推理能力，但其全部潜力可以通过提示过程中的精心指导和结构得以释放。

#### Agentic Workflow

除了大语言模型（LLMs）的指令遵循和上下文学习能力外，研究人员开始设计代理工作流，以编程LLMs的“思维模式”[137]。这种代理工作流使研究人员能够在不进行额外训练的情况下增强LLMs的推理能力，但通常需要更多的测试计算资源。上下文学习[33, 25]是指通过提供少量上下文示例来提高LLMs在特定任务中的表现的能力，从而使LLMs能够有效地泛化到未见问题，而无需进行计算成本高昂的训练[14]。尽管这种能力的起源仍然是一个有争议的话题，但最近的研究表明，上下文学习通过让LLMs捕捉标签空间、输入文本的分布以及答案的期望格式，从而提升了其性能[97]。这些理想特性使研究人员能够将通用LLMs适应于多样化的任务场景，例如通过上下文角色扮演模拟特定人口群体的视角[22]。最近的研究表明，有效的代理工作流可以显著提升LLMs在模拟人类行为[105, 127]、人机交互[89]以及协作任务解决[107]方面的能力。通过代理工作流编程LLMs的能力，为利用复杂认知架构提升LLMs的推理能力奠定了基础。

### Data Construction: from Human Annotation to LLM Automation

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=MDFmNzc1NjgwNzcwN2VlODRhZTQ1NjZkYmQxNDhkZjRfN3FQVUlWQ2xyMlp4cXJWMVdXOGhwZWdCRmdnTU1nU2FfVG9rZW46UzdxNmIzeVNDb2tUakl4aFJZM2NxRm5YbkZ1XzE3NDg3NTkyMjk6MTc0ODc2MjgyOV9WNA)

创建大规模、高质量的推理数据集对于提升大型语言模型（LLMs）的推理能力至关重要。然而，这一任务由于成本高昂而面临重大挑战。如图1所示，人工标注被广泛认为是高质量的，但其成本极高且难以扩展。相反，利用LLMs自动化标注过程提供了一种更具成本效益的替代方案，但面临着验证有限的挑战，特别是在逐步推理过程中。在本节中，我们回顾了该领域的最新研究进展（总结见表1），强调了从人工标注向LLM自动化的转变。

#### Human Annotation

人类标注在构建大型语言模型（LLMs）数据集中的作用是不可或缺的。人类标注者以其细致、耐心和精确性为特征，同时具备适应新场景的能力，并能有效处理模糊数据 [98]。Zhou 等人 [195] 的研究表明，即使使用最少的人类标注数据，模型也能表现出强大的性能，这凸显了精心策划的标注在模型有效性中的关键作用。人类标注的数据在增强大型语言模型的推理能力方面发挥了重要作用。在基于人类反馈的强化学习（RLHF）[104] 的背景下，来自人类标注者的偏好数据使得最初在通用文本语料库上训练的 LLMs 能够与复杂的人类价值观和伦理考量保持一致。这种可推广的标注方法有助于针对特定任务微调模型。在此基础上，Lightman 等人 [75] 展示了使用人类标注者评估数学推理过程中每一步推理质量的有效性，显著提高了 LLM 推理的准确性。这突显了人类标注如何弥合通用训练数据与领域特定挑战（如复杂推理任务）之间的差距。

  

提升大型语言模型（LLMs）的推理能力需要过程监督，即由人类标注者指导推理过程的每一步[75]。然而，这种监督需要大量的人工标注数据，既耗费资源又难以持续。鉴于LLM训练通常需要数TB的数据，且数据量对模型性能至关重要，完全通过手动标注构建数据集变得越来越不切实际。这凸显了在不完全依赖人工标注的情况下改进推理能力的替代方法的必要性。一种有前景的方法是人与LLMs协作进行标注，利用LLMs加速标注过程，同时保持人工标注的高质量。具体而言，标注过程可以分为两个阶段：预标注阶段和精炼阶段。在预标注阶段，可以利用LLMs进行初步标注，借助少量手动提供的示例快速高效地完成设置[42, 61]。在精炼阶段，人类标注者可以评估LLMs生成的标注质量，并专注于修正质量较差的标注子集[61, 152, 96, 42]。为了实现可扩展的标注过程，最近的研究越来越关注如何在确保数据质量的同时最大化自动化，从而在不影响标注准确性的前提下减少人类参与。

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=ODU1YWIwMzcxZTdlNmI4NGEzZTg2MjAzMDEyNWIwNTNfTzFCOUI5bGtXSVRpRzRZNFo4TFBXRGNUV2dZNnhRZnBfVG9rZW46WGhXaWJ2Yndyb2wyMjB4NWM5TWNBUU5wbm5kXzE3NDg3NTkyMjk6MTc0ODc2MjgyOV9WNA)

#### LLM Automated Outcome Annotation

数据标注是一项具有挑战性且资源密集型的任务，尤其是在需要复杂操作（如过滤、识别、组织与重构文本数据）的场景中。这些任务通常繁琐、耗时，并且需要大量的人力投入，使其成为大规模数据构建过程中的昂贵瓶颈[142, 31]。为了解决这些挑战，利用大语言模型（LLMs）进行数据标注提供了一种经济高效且便捷的替代方案。凭借超过10万标记的上下文窗口长度，LLMs能够轻松处理长文本和大量结构化数据[2]，以显著的效率满足数据标注的复杂需求。其强大的指令遵循能力[187]使其能够灵活适应多样且复杂的标注场景，同时达到与人类标注者相当的质量水平。通过自动化这些高要求的任务，LLMs显著减少了对人力的依赖，优化了标注流程并提升了整体生产效率[181]。LLMs能够处理多种自动化标注任务，从简单的问答提取[106]到包含额外目标信息[161]的任务。在没有人类示范的情况下，LLMs依靠其强大的推理能力和上下文学习能力独立应对更复杂的标注需求。

  

大型语言模型（LLMs）能够处理多种自动化标注任务，从简单的问答提取[106]到包含额外目标信息的任务[161]。在没有人类示范的情况下，LLMs依赖其强大的推理能力和上下文学习能力，独立应对更复杂的标注需求。例如，Schick等人[120]展示了如何利用LLMs构建工具使用的数据集。对于每个可能需要API调用的候选位置，LLMs能够理解上下文中的逻辑关系，生成相关问题，并识别出合适的工具API来解决问题。当有人类示范时，LLMs可以通过模仿这些示例中的模式和推理策略来进一步提升其表现。对于复杂任务，人类示范提供了高质量的轨迹——即一系列思想、观察或行动——这些轨迹指导LLMs复制人类的决策过程。现有研究表明，即使是在零样本情况下，基于人类示范的任务无关提示也能有效引导LLMs完成标注任务[65]。此外，对于涉及高度复杂和微妙轨迹的任务，LLMs可以引入专门的代理，如计划代理、工具代理和反思代理，以处理标注过程的不同方面，从而进一步增强其与人类推理和行为对齐的能力[109]。这些多样化的能力自然延伸到推理结果标注任务中，LLMs不仅能够推断潜在的逻辑结构，还能系统地记录中间推理步骤及其相关结论。这使得创建的数据集不仅捕捉最终结果，还捕捉导致这些结果的完整推理过程，为下游应用提供了更丰富的见解。

除了通过人类示范进行标注外，大型语言模型（LLMs）还可以通过带有反馈的搜索独立增强其标注能力。这一过程涉及从动态环境中学习并进行迭代优化。失败的数据点可以被视为一种经典的反馈形式，为模型提供了识别弱点并设计针对性调整的宝贵信息。通过自我纠正错误样本并生成优化的训练数据，LLMs进入了一个自我提升的循环，从而增强了其理解和推理能力[70]。此外，LLMs能够系统分析其错误的原因，提取关键见解并将这些编码为自我学习的知识，以指导未来的推理任务[72]。这种基于反馈的方法还可以通过将失败的轨迹与基于相似性的成功轨迹配对，采用对比学习策略来优化模型参数。通过这种迭代搜索和优化机制，LLMs不仅能够纠正错误，还能发展出更强大的推理能力，从而在复杂任务中实现更深入的泛化和适应性[135]。

#### LLM Automated Process Annotation

在复杂的推理任务中，模型输出的每一步都可能对最终结果产生重大影响，因此有必要将中间决策标记为“正确”、“错误”或分配中间奖励，即过程标注。然而，手动标注这些步骤既昂贵又耗时。例如，Lightman等人[75]投入了大量人工努力，生成了大规模的过程标注数据集PRM800K，该数据集满足了训练有效过程奖励模型（PRM）的需求，并显著提升了大型语言模型（LLMs）的推理能力。因此，自动化方法的需求日益增加，以实现高效的过程标注，确保可扩展性和成本效益。最初的自动化方法利用外部更强的LLMs来标注由较小LLMs生成的中间过程。此外，基于蒙特卡罗的方法减少了对外部更强LLMs的依赖，能够使用较弱的LLMs完成数据标注，从而通过自我强化的方式训练出更强的LLMs。

  

**使用更强大的LLM进行标注**：作为一种直接的自动化标注方法，Luo等人[84]提出利用更强大的外部模型对生成模型推理过程的中间结果进行标注。该方法不依赖于人工标注，而是使用预训练的高性能模型（如GPT系列）来评估每个生成步骤。通过利用更强大外部模型的能力，这种方法提高了标注过程的准确性和可扩展性，使其更适用于大规模任务。然而，该方法的主要局限性在于其对高性能外部模型的依赖，这意味着标注过程的性能最终受限于所使用外部模型的能力。

  

**通过蒙特卡罗模拟进行注释：**为了减少对强大外部模型的依赖，Wang等人[148]和Wang等人[156]提出了一种改进的方法，该方法避免直接对中间步骤进行评分。相反，他们的方法使用外部模型从给定的中间输出继续推理若干步骤，并多次随机重复这一模拟过程。然后，基于这些扩展推理的平均结果来评估中间步骤的质量。这种蒙特卡罗方法在数学问题求解和代码生成等任务中显示出良好的效果。

  

**基于树搜索模拟的注释方法**：通过使用多步蒙特卡洛模拟并结合外部模型来评估中间步骤的质量，基于平均结果的方法已成为自动化过程注释中最广泛使用的技术之一。为了进一步提升该方法的效率，Luo等人[85]提出了一种改进方案，即用蒙特卡洛树搜索（MCTS）策略替代重复的蒙特卡洛模拟。在这一改进方法中，通过MCTS从中间步骤生成多个代表最终推理结果的叶节点，并基于这些叶节点的平均结果来评估中间步骤的质量。与随机重复推理相比，MCTS利用树搜索提高了推理质量，同时允许叶节点共享高质量的父节点，从而减少了计算开销并提升了效率。该方法在数学问题求解中表现出色，甚至超越了人工注释的性能。

  

在基于蒙特卡洛树搜索（MCTS）的模拟基础上，Zhang等人[183]进一步引入了自优化机制到过程标注中。他们利用获得的过程标注来训练过程奖励函数（PRM），从而提升大语言模型（LLM）的性能。优化后的LLM再次用于基于MCTS的模拟，生成更高质量的标注。这一迭代过程通过多次循环改进，逐步提升了过程标注的质量。该方法在多个任务中表现出色，包括数学问题求解、问答以及多领域知识推理，展示了其通过迭代增强不断优化和提升标注质量的有效性。

### Learning to Reason: from Supervised to Reinforcement Fine-tuning

尽管预训练模型在各种任务中表现出色，但它们通常在复杂推理和输出与人类期望的对齐方面存在困难。微调对于解决这些局限性至关重要，它能够优化模型在特定任务上的表现，并增强其推理能力。最初，使用监督微调（SFT），模型从标注数据集中学习任务特定的模式。然而，随着推理挑战的增加，强化学习（RL）和直接偏好优化（DPO）等方法提供了更有效的途径，通过奖励模型更高效地将模型输出与类人推理对齐，从而生成更连贯、负责任且具有上下文感知的输出。

  

#### Optimizing Pre-trained LLM: Supervised Fine-tuning

监督微调（Supervised Fine-Tuning）是一种学习技术，它通过使用标注数据来优化预训练模型在特定任务或领域中的能力，同时保留模型对预训练知识的理解。尽管预训练使模型能够从大量非结构化数据中学习广泛且通用的特征，但微调通过让模型接触较小且具有明确输入输出映射的任务特定数据集，使其更加专业化。

SFT 是提升大语言模型（LLMs）推理能力的关键步骤，通过将其从通用系统调整为领域专用工具，使其能够应用于下游任务。例如，像 GPT [111]、BERT [30] 和 T5 [113] 这样的 LLMs 通过自监督学习在大量文本数据上进行预训练，使其具备了广泛的语言理解和生成能力。然而，它们的输出并不总是符合任务特定要求。如果没有微调，LLMs 在某些推理任务（如物体计数 [182]、卫星理解 [91] 和工程问题回答 [154]）上往往表现不佳。通过 SFT，我们可以基于标注的任务特定数据集优化模型的输出，从而部分解决这些挑战。

然而，直接应用监督微调（SFT）可能无法充分挖掘模型在特定领域中的推理能力，尤其是在需要更复杂决策或多步问题解决的任务中。引入思维链（CoT）技术[160]彻底改变了SFT过程，通过明确训练模型在得出答案之前生成中间推理步骤。基于CoT的SFT鼓励大语言模型（LLMs）明确生成中间推理步骤，从而增强其处理需要更结构化、条理化思维任务的能力。例如，ReasonBert[29]表明，通过引入逐步推理过程对模型进行微调，可以显著提升其在数学应用题和逻辑推理等任务中的表现。

另一项关键研究[80]探讨了如何通过推理来微调模型以提高其可解释性，并在复杂的决策场景中通过生成更透明、逐步的思维过程减少错误。通过使用CoT（Chain-of-Thought）进行微调，模型不仅改进了最终答案，还增强了其“逐步思考”问题的能力，提供了对模型推理过程更清晰的洞察。

尽管SFT（监督式微调）方法多样且性能出色，但它也存在一些局限性。首先，SFT严重依赖高质量标注数据集，这些数据集的整理可能既昂贵又耗时，尤其是在小众领域或需要专家注释的任务中。其次，SFT可能导致灾难性遗忘，即在微调过程中，模型可能会丢失一些预训练时获得的通用知识，从而降低其在微调领域之外的任务中的实用性。最后，即使采用参数高效的方法，大规模模型的微调计算成本仍然很高，这对资源有限的组织构成了挑战。解决这些局限性需要谨慎的数据集整理、正则化技术以及探索替代方法，如提示调优或多任务微调，以平衡任务专业化和泛化能力。

#### Optimizing Pre-trained LLM: Reinforcement Learning

  

由于对昂贵的高质量标注数据集的高度依赖以及监督微调（SFT）的高计算成本，强化学习（RL）已成为训练模型掌握推理过程的一种强大替代框架。与监督学习不同，强化学习使模型能够通过试错和奖励信号进行学习，从而发现实现特定目标的最佳策略。如图2（a）所示，模型根据其当前状态采取行动，并以奖励信号的形式接收反馈。这种反馈引导模型随时间更新其参数，以优化累积奖励。

  

**经典强化学习**。强化学习（RL）已成为大型语言模型（LLMs）发展中的关键步骤。在RL框架中，LLMs的参数根据其行为所获得的奖励进行更新。具体来说，价值函数或Q函数根据奖励模型的反馈进行更新，将行为结果的全部功劳归因于其即时效果。这种方法简化了框架，使其在概念上更加直观，同时增强了模型有效响应的能力。目前，两种主要方法主导了LLMs的RL训练：**基于人类反馈的强化学习（RLHF）******和******基于AI反馈的强化学习（RLAIF）**。

Ouyang等人[104]使用RLHF将LLMs与人类意图对齐。此外，通过对GPT-3进行人类标注的演示和排序比较的微调，他们开发了一个预测人类标注者偏好的奖励模型。这有效地将训练后的LLMs与人类偏好对齐，尽管模型规模较小，但在推理和指令遵循方面优于GPT-3。Bai等人[8]也利用RLHF创建了有帮助且无害的语言模型。遵循“有帮助、诚实、无害”的框架，他们对基础模型进行微调，使用拒绝采样训练偏好模型，并通过人类反馈进行迭代优化。这一过程生成的AI助手在自然语言处理任务中表现出色，并展现出强大的伦理推理能力。

  

为了减少对大规模人工标注数据的依赖，Bai等人[9]提出了**Constitutional AI**，这是一种通过原则而非昂贵的人工反馈来训练AI助手使其既有用又无害的框架。该过程包括两个阶段：监督学习和RLAIF（基于强化学习的AI反馈）。在监督学习阶段，模型根据宪法原则对其输出进行批判和优化，从而创建一个微调数据集。在RLAIF阶段，模型生成自我评估以指导训练，从而绕过对人工标注有害性数据的需求。Ramamurthy等人[114]则专注于使用强化学习（RL）来使大语言模型（LLMs）与人类偏好对齐。他们提出了**RL4LMs**，这是一个用于基于RL的微调的库，以及**GRUE**基准，该基准使用反映人类偏好的奖励函数来评估模型。为了解决训练中的挑战，他们提出了自然语言策略优化算法，通过约束标记采样来稳定训练。这项工作为将RL整合到LLM微调中以提高对齐性和性能提供了坚实的基础。

  

  

**直接偏好优化（Direct Preference Optimization, DPO）**，用于对齐语言模型与人类偏好。传统的强化学习（RL）方法依赖于训练奖励模型来根据人类偏好对输出进行评分，而DPO则通过直接利用偏好数据来简化这一过程，无需显式的奖励模型。DPO不优化复杂的奖励函数，而是使用成对偏好比较，即数据表明人类偏好两个输出中的哪一个。这种直接的方法简化了学习流程，同时保留了基于RL方法的一致性优势，通常更简单且更有效。

Rafailov等人[112]提出了DPO，这是一种新颖的框架，用于对齐语言模型，通过简单的分类损失直接优化策略以与人类偏好对齐。通过参数化奖励模型以推导出闭式最优策略，DPO在微调过程中消除了采样和大量超参数调整的需求。实验表明，DPO在情感控制、摘要和对话生成等任务中与或优于RLHF方法（如PPO），同时更稳定、计算效率更高，并且在生成推理输出方面更有效。

Amini等人[4]提出了带有偏移的直接偏好优化（ODPO），这是DPO的扩展，用于将语言模型与人类偏好对齐。ODPO通过考虑响应之间的偏好程度而不是平等对待所有偏好对来改进DPO。它在偏好和非偏好响应的似然差异中引入了一个偏移，与它们的质量差异成比例。这种方法不仅提高了一致性，还增强了模型的推理能力，特别是在情感控制、毒性减少和摘要等任务中。实验表明，ODPO在偏好数据有限的情况下，能够实现更好的一致性和负责任的行为。

在结论中，RL（强化学习）和DPO（直接偏好优化）方法为提升大型语言模型（LLMs）的推理能力提供了一种直接且有效的方法。通过关注每个动作后的即时奖励，这些方法也使模型与人类偏好保持一致。对短期反馈的强调简化了学习过程，避免了在长序列中进行信用分配的复杂性。这种简化的方法特别适合实时应用和需要清晰、简洁推理的任务，最终增强了LLMs提供连贯和合乎道德结果的能力。

#### Enhancing Multi-step Reasoning with Outcome Reward Model

对于复杂的推理任务，如数学问题解决，大型语言模型（LLMs）需要进行多步推理，如思维链（Chain-of-Thought），以最终达到准确的解决方案。在这些任务中，奖励反馈通常只有在所有推理步骤完成并获得最终解决方案后才能获得。如图2（b）所示，这被称为结果奖励模型（Outcome Reward Model, ORM）。在这种情况下，提高LLM推理能力的关键在于根据结果奖励区分中间推理步骤的正确性和重要性。

  

**经典强化学习。**ReFT [143] 将 RLHF [104] 中的 PPO [121] 方法应用于推理任务。基于结果奖励模型，PPO 中的价值函数能够推断中间推理步骤的贡献。与监督微调相比，ReFT 能够学习更多样化的推理路径，在推理任务中表现出更强的泛化能力。然而，VinePPO [60] 发现，使用 ORM 训练的 PPO 价值网络在识别中间推理步骤的价值时表现出显著偏差，这是强化学习中一个众所周知的挑战，称为信用分配问题。为了解决这个问题，VinePPO 放弃了 PPO 中的价值网络，转而采用蒙特卡洛采样方法来计算价值函数的无偏估计。实验结果表明，VinePPO 在数学推理任务中始终优于典型的 PPO。关键计划步骤学习（CPL）是一种旨在通过在高层次抽象计划中搜索来增强 LLM 在推理任务中泛化能力的方法 [150]。CPL 采用蒙特卡洛树搜索（MCTS）来探索多步推理任务中的不同计划步骤，并利用 Step-APO 来学习关键计划步骤。这种方法使模型能够学习更多样化的推理路径，从而提高在各种任务中的泛化能力。随后，模型迭代训练策略和价值模型以进一步提高性能。在每次迭代中，策略模型生成计划步骤和最终解决方案，而价值模型评估中间步骤的质量。由 MCTS 生成的训练数据用于更新策略和价值模型。

  

在数学推理任务中，直接使用DPO（直接偏好优化）方法进行偏好优化会由于偏好数据中存在较长的推理步骤而导致次优结果。Amini等人提出了ODPO（优化直接偏好优化），通过考虑响应之间的偏好程度而不是将所有偏好对视为平等，对DPO进行了改进。ODPO在数学推理任务中相比DPO取得了显著的改进。总的来说，基于结果奖励的训练主要挑战在于区分中间推理步骤的正确性和重要性。当前的方法主要基于蒙特卡洛采样或蒙特卡洛树搜索，在估计这些中间步骤的重要性方面具有优势，尽管搜索过程中的计算成本仍然较高。现有的工作主要集中在数学或其他推理问题上，这些问题的最终解决方案可以轻松验证。这些方法可以扩展到更广泛的推理任务，包括那些解决方案难以验证的任务。一个潜在的方法是学习一个基于人类标注数据的奖励模型，并使用它来判断最终解决方案的质量。基于奖励模型提供的最终分数，可以进一步使用蒙特卡洛采样或搜索技术来提高性能。

#### Enhancing Multi-step Reasoning with Process Reward Model

Process Reward Model (PRM) 基于强化学习代表了大型语言模型（LLM）推理领域的重大进展，它强调对中间步骤的评估，而不仅仅关注最终状态的结果。如图2（c）所示，PRM的奖励分布在每个推理步骤中，而不是集中在最终结果上。通过在推理轨迹中提供细致的反馈，PRM使模型能够更好地优化行为，使其更符合人类偏好和复杂任务需求。这种方法对于涉及顺序决策的任务至关重要，其中中间步骤或决策对最终目标具有重要意义。我们探讨了PRM的演变，并强调了其在复杂任务中通过提供步骤级奖励来改进推理的作用。

  

**经典强化学习** 最近一系列的研究将过程奖励模型（PRMs）应用于数学或逻辑推理，因为OpenAI的一项开创性工作[75]已经证明了过程奖励的重要性。SELF-EXPLORE [55] 使用PRMs来增强数学推理，通过识别和解决“第一坑”（即问题解决中的初始错误步骤）。通过奖励纠正这些错误的步骤，PRMs使得无需大量人工标注即可进行自我监督的微调。该模型通过利用步骤级别的细粒度反馈，在GSM8K和MATH等数学基准上实现了显著的准确性提升。MATH-SHEPHERD [149] 引入了一个PRM框架，专为数学推理任务中的逐步验证和强化而设计。通过受蒙特卡洛树搜索（MCTS）启发的方法自动化过程监督，MATH-SHEPHERD消除了对人工标注的需求，同时确保了多步问题解决的高准确性。PRMs被用于强化逻辑进展和正确性，从而在GSM8K和MATH等基准上提升了性能。DeepSeekMath通过组相对策略优化（GRPO）[128]（一种优化步骤级别奖励的强化学习算法）整合了PRMs。PRMs被用于增强数学推理和跨领域的推理一致性。通过关注中间推理步骤，DeepSeekMath在多个基准上实现了最先进的性能，展示了PRMs在数学领域的强大能力。扩展自动过程验证器引入了过程优势验证器（PAVs），一种PRM变体，用于评估问题解决中的步骤级别进展[123]。PAVs使用步骤级别的监督来提高搜索算法和强化学习的效率和准确性。通过关注那些朝着正确解决方案迈出有意义进展的步骤，PAVs在样本效率、计算效率和推理准确性方面相比结果奖励模型实现了显著提升。这证明了细粒度过程奖励在扩展大语言模型推理能力中的重要性。

  

**交互式过程奖励模型**。PRMs（过程奖励模型）也被应用于交互式任务，如对话和多轮问答。ArCHer采用了一种基于PRMs的分层强化学习方法，用于训练处理多轮、长周期任务的智能体[198]。它实现了一个双层系统：高层价值函数评估话语级别的奖励，而低层PRM则优化每轮中的逐令牌生成。这种分层结构确保了更有效的信用分配，并允许对语言模型进行细致训练，以处理多轮交互和推理任务。PRMs的使用使ArCHer能够高效扩展，在智能体任务中显著提升了样本效率和性能。**基于偏好反馈的多轮强化学习**[126]将PRMs整合到多轮强化学习中，以优化长期目标并融入人类反馈。多轮偏好优化（MTPO）算法通过比较整个多轮交互来生成偏好信号，其中PRMs用于分配逐步奖励。这使得LLM（大语言模型）智能体能够将行为与长期目标对齐，从而在动态的多轮任务（如对话和战略决策）中提升整体表现。

  

**直接偏好优化**。最近的一些研究利用蒙特卡洛树搜索（MCTS）通过直接偏好优化（Direct Preference Optimization, DPO）来实现多步推理任务的优化[165, 17, 183, 16]。例如，SVPO[17]使用MCTS自动为多步推理任务标注步骤级别的偏好。从学习排序的角度来看，它训练了一个显式的价值模型来复制隐式奖励模型的行为。此外，SVPO将显式价值模型与DPO相结合，其中价值模型不仅帮助策略模型导航更高效的推理路径，还指导偏好学习。然而，这些工作主要集中在首先收集偏好数据或训练奖励模型，然后基于静态数据和预训练的奖励模型进行策略优化。Xie等人[165]通过将数据收集和策略偏好优化整合到一个迭代过程中，推进了这些方法。这种方法可以被视为直接偏好优化的在线版本，其中更新的策略被迭代地用于通过MCTS收集偏好。

多步强化学习技术在大语言模型（LLMs）中的演变反映了从稀疏的结果反馈到详细的过程导向监督的转变。过程奖励模型（PRMs）现在成为LLM推理能力进步的核心，提供了细致的步骤级别奖励，从而在推理任务中实现了显著改进。未来的研究可能会集中在优化这些模型并扩展其在不同任务领域的适用性。

#### Reinforcement Fine-tuning

  

Reinforcement fine-tuning (RFT) [101] 是 OpenAI 最近提出的一种技术，用于定制专门针对特定垂直领域的专家级大型语言模型（LLMs）。目前，RFT 仍属于研究项目的一部分，技术细节尚未完全公开。现有信息表明，RFT 利用用户提供的少量偏好数据以及一个评分模型来评估 LLM 的输出。该技术能够迭代优化 LLM 的多步推理能力。因此，RFT 技术可以增强 LLM 在优化领域中处理类似问题的推理策略。

**评分模型**。RFT引入了评分模型的概念，用于评估大语言模型（LLMs）的输出。考虑到强化学习训练通常需要一个奖励模型来提供反馈，评分模型很可能类似于奖励模型，将文本输入（例如问题和答案）转换为推理质量的标量值。这表明评分模型可以作为一个基于用户提供的偏好数据训练的奖励模型，可能作为结果奖励模型或过程奖励模型运行[76]。

**数据效率**。在OpenAI的直播会议中，提到RFT可以在仅需几十个用户偏好数据的情况下，在新领域中进行学习。这表明RFT能够基于有限的偏好数据探索多样化的推理路径来解决任务。这种方法展示了极高的样本效率，同时降低了过拟合的风险[56]。

**训练稳定性**。强化学习训练的稳定性是一个众所周知的难题，对其广泛应用提出了重大挑战。随机种子的变化或某些超参数的调整会极大地影响RL的训练结果。在RFT项目的背景下，OpenAI宣布计划通过API向公众提供这项技术，使用户能够使用自己的数据微调特定领域的专家模型。这一声明可能表明RFT已经达到了足够的稳定性，能够可靠地使用RL技术微调语言模型。

### Test-time Scaling: from CoTs to PRM Guided Search

#### Elicit Deliberate Thinking with Prompts

在训练时优化技术（如强化学习）之外，研究人员发现测试时提示技术（如Chain-of-Thought和Tree-of-Thoughts）可以进一步增强大语言模型（LLMs）的能力 [160, 153]。虽然直接要求模型给出答案通常会产生次优结果，但在测试时通过明确的推理过程引导它们，可以显著提高其性能 [62]。这些提示策略在从数学推理到复杂决策任务的各种领域中表现出显著的有效性 [173, 196]。结构化提示方法（如ReAct和Least-to-Most Prompting）的出现表明，LLMs可以从明确的指导中受益，以组织其思维过程，从而产生更可靠和可解释的输出 [189]。尽管这些方法通常会增加令牌消耗和计算开销，但它们通过增强LLMs的推理能力和解决方案的准确性，而不需要修改模型参数，为训练时方法提供了有力的补充 [172, 11]。这表明，通过复杂的测试时干预来提升LLM性能，而不是仅仅依赖模型架构或训练修改，是一个有前途的方向。

#### PRM Guided Search

正如之前提到的，PRM（过程导向反馈模型）标志着从稀疏的结果导向反馈向详细的过程导向监督的重大转变。更重要的是，PRM还可以在测试阶段使用，进一步提升模型的推理能力。OpenAI的o1系列模型是PRM先进应用的一个突出例子。新的测试阶段扩展法则表明，通过增加测试阶段的计算资源，可以有效地增强推理能力，这为未来大型语言模型（LLMs）的发展提供了明确的方向。我们介绍了一些在推理阶段应用的方法，如图3所示。红色空心圆代表在推理阶段算法探索过程中被丢弃的推理路径，绿色空心圆表示在探索过程中被采纳的推理路径，而绿色实心圆则标志着一旦找到正确答案，推理路径的终点。

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2FlMTgwOTNiMmI2ZGVmYTUzMWU1ZmFjODFjYWUwMzhfcjJUbFVHdmNaNkdwQU1ER242c2gxcXQ1WGFQUW9VbFdfVG9rZW46TnBHMGIzQlEwb1h0SEd4NmY3R2NDRHdubldkXzE3NDg3NTkyMjk6MTc0ODc2MjgyOV9WNA)

  

  

**多数投票（Majority Vote）**是一种从密集测试时间计算中生成最终答案的最直接策略。在推理过程中，每个推理轨迹都会对给定输入产生一个预测。其基本思想是选择大多数推理轨迹一致认同的答案。然后，将所有模型的预测进行聚合，选择出现次数最多的类别（即“多数投票”）作为最终输出：

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDlkZGIxZTFlMTQ0ZWM2ZmY2Y2Q3NmE2YTg1ZjI4MjVfeG1FWElPdXpJVGJ4cWUwOHBNNWpTcElJcDg0Sno4T2dfVG9rZW46VEhtUmJad0ZFbzdBMlV4Sk91eGNMWVBCbnBiXzE3NDg3NTkyMjk6MTc0ODc2MjgyOV9WNA)

其中I是指示函数，y是每个评估轨迹。

  

**树搜索 [15]**：树搜索是一种经典算法，通过递归构建搜索树来系统地探索不同的选择。它通常用于复杂的决策问题，例如棋盘游戏和规划任务。蒙特卡罗树搜索（MCTS）是最广泛使用的树搜索方法之一。它由四个主要步骤组成：选择、扩展、模拟和反向传播。通过逐步扩展搜索空间，MCTS逐步改进决策。树搜索已经在一些大型语言模型（LLM）推理任务中得到了应用，并取得了显著的成果。例如，Tree-of-Thoughts框架 [172] 使LLM能够考虑以树结构组织的多种推理路径。它通过自我评估来做出深思熟虑的决策，确定下一步的最佳行动方案。这种方法显著提高了模型推理的性能。

**Beam Search [133]**：Beam Search 是贪婪搜索的改进版本，常用于生成任务中以选择最优的输出序列。其主要思想是在每个时间步从所有候选路径中保留得分最高的前K条路径（称为 beams）以进行进一步扩展。与贪婪搜索不同，Beam Search 保留了多个候选路径，从而扩展了搜索空间并提高了生成质量。Beam Search 在LLM推理中得到了广泛应用。例如，BART [71] 使用 Beam Search 作为其主要推理策略，展示了其在文本生成任务中的出色效果。

**Lookahead Search [134]**：Lookahead Search 是另一种有潜力显著增强大语言模型（LLM）推理的优化方法。它修改了Beam Search（束搜索）中每一步的评分机制。与仅根据当前步骤的分数选择最佳候选方案不同，Lookahead Search通过向前模拟最多k步来进行决策。如果在向前模拟过程中达到解决方案的终点，则提前终止该过程。在Lookahead Search中，使用一个预训练且固定的预测奖励模型（Predictive Reward Model, PRM）来对模拟的每一步进行评分。通过PRM在k步模拟中获得的累积分数，决定是否保留或丢弃某个束分支。这种策略通过在每个评估步骤中融入更多上下文信息来改进决策。与束搜索相比，Lookahead Search增加了探索空间的深度，使得能够基于更远距离的模拟决策结果来评判当前的决策。然而，这也增加了对计算资源的需求，在计算资源有限的情况下可能会导致性能下降。

  

### Path toward Large Reasoning Model

#### Development of OpenAI o1 Series

在2024年9月，OpenAI发布了o1，这是一款突破性的语言模型，代表了人工智能推理能力的重大进步，尤其在数学、编程和科学问题解决等复杂任务中表现出色。2024年12月20日，OpenAI开放了o3的测试申请，o3是o1的升级版本[102]，被认为具有相当于博士水平的智能[7]。这些模型在各种具有挑战性的基准测试中取得了显著成果，包括在国际数学奥林匹克竞赛中获得金牌水平[73]，并在物理、化学和生物问题上达到博士级别的表现[48]。通过对其基本推理能力的系统分析，广泛评估展示了o1系列独特的推理模式。我们将现有研究的关键发现列举如下：

  

  

  

**有效的知识整合。**初步的综合评估[194]展示了o1在基础问题解决任务中的结构化分析方法和知识整合能力，通过逐步的逻辑推理，在竞争性编程中达到了83.3%的成功率，模型展示了清晰的能力，能够利用其知识分解复杂问题并遵循正式的推导过程。模型的结构化理解和互联知识应用在放射学和芯片设计等专业领域进一步得到证明，这些领域需要整合多个领域概念来进行准确的诊断和复杂的电路分析。系统评估[68]定量验证了这种模式，显示在结构化分析思维和计算推理任务中，模型的表现达到了人类水平的150%。这种优势在需要跨领域知识整合的场景中尤为突出，例如将物理原理应用于生物系统或将统计方法与领域特定约束相结合，表明模型在知识综合和应用方面具有基本能力。

  

  

**系统化问题分解。**o1在不同复杂度的任务中保持一致的性能，显示出在处理难度增加时的系统化问题分解能力。在数学推理中，详细研究[27]展示了其系统化问题分解的方法，通过结构化的解题步骤在荷兰数学B考试中取得了接近满分的成绩。该模型展示了识别关键数学原理、构建形式化证明以及逐步验证解决方案有效性的能力。这种一致性在更复杂的场景中得到了验证，如研究[26]对105个难度递增的科学和数学问题的验证，该模型在概念深度和计算需求增加的情况下仍保持高准确性。在编程任务中，这种模式通过QuixBugs基准测试中的系统化调试[52]进一步得到证明，o1通过结构化的三步方法（错误识别、根本原因分析和针对性修正）在不同复杂度的错误中保持一致的性能。

  

**在复杂任务中，模型展现出可靠且连贯的推理能力。**其推理能够有效适应不同类型的问题，始终在各种任务中保持推理链的一致性。在规划任务中，PlanBench评估[144]表明，模型能够系统性地处理确定性和概率性场景，在约束满足和状态管理方面表现出显著改进。模型在处理信息不完整和动态约束的问题时表现出特别优势，在标准和罕见任务变体中均保持一致的性能[94]。这种适应性表明模型在不同问题表述中具有强大的泛化能力。关于复杂规划的研究[146]进一步展示了模型在长周期任务中保持推理连贯性的能力，能够有效管理扩展的依赖链和上下文转换。这一点在解决多步规划问题时尤为明显，其中中间目标必须正确排序，依赖关系需要仔细管理，展示了模型在时间推理和因果理解方面的高级能力。

  

  

**新的大规模推理模型的扩展定律**

实证研究表明，o1 在训练和推理阶段展现出独特的扩展模式。在训练过程中，模型的大规模强化学习算法通过高效的数据利用，教会其使用“思维链”进行高效思考 [103]。研究 [134] 表明，通过优化的测试时间计算策略，模型在各种推理任务中实现了显著的性能提升。综合评估 [194, 68] 显示，o1 的推理能力可以通过推理阶段的高级计算分配得到有效增强，尤其是在复杂问题解决场景中。这种扩展方法的约束条件与大型语言模型（LLM）预训练有显著不同，其性能随着思考时间的增加而持续提升 [103]。这在编程任务中得到了验证，允许每个问题提交 10,000 次时，模型能够显著提升结果，即使没有测试时间选择策略，也能达到金牌阈值以上。模型在训练和推理阶段有效利用额外计算资源的能力，表明了推理架构的根本性进步，在传统方法可能需要更大模型规模的场景中表现出特别的优势。

#### Open-source Attempts of Large Reasoning Models

开源框架在开发大型语言模型（LLMs）的高级推理能力方面也取得了显著进展。这些框架为研究人员和开发者提供了宝贵的参考，旨在复制或近似于像OpenAI的o1这样的专有模型的推理优势。在本节中，我们介绍了四个重要的开源项目，每个项目都采用了不同的策略来增强LLM的推理能力（总结见表2）。通过探索它们的独特实现，我们旨在提供关于强化LLM推理能力的多样化方法的见解。

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=NTk4YWI0MjFhY2ZlNDhlYzQwM2Q0Mzc4OTk5YjAyM2RfQlM3UXh0VTZoMWlmUHJJcThRQkJ2eWNsTlBUVWZrUmpfVG9rZW46V0ZldmJnN1FOb2lHZld4aTVqVWNWNXVpbmFkXzE3NDg3NTkyMjk6MTc0ODc2MjgyOV9WNA)

  

**OpenR项目[145]**。该项目声称是首个利用强化学习技术探索OpenAI o1模型核心方法的开源框架。OpenR复现的核心在于构建逐步推理数据，从而获得更为精确和细粒度的反馈，而不仅仅是最终答案。通过从构建的搜索树中选择推理轨迹，采用了自动化数据增强算法OmegaPRM [85]。基于对每个推理步骤的监督增强过程数据，进一步在预训练的Qwen2.5-Math-7B-Instruct模型[168]基础上，以监督学习方案训练了一个过程奖励模型（PRM）。该PRM可直接在测试时计算中部署，并与多数投票、最佳N选或束搜索方法集成。它还可用于在训练后阶段通过强化学习微调大语言模型（LLM）。实验证明了PRM在测试时计算和训练后阶段的有效性。

**Rest-MCTS[183]** 并没有分别训练 PRM 和微调的策略模型，而是将这两个更新过程整合到一个相互自训练的循环中。基于类似设计的 MCTS 算法，预先收集了作为 PRM 训练监督的过程奖励和用于策略模型训练的推理轨迹。随后，基于初始策略和初始 PRM 值 V，迭代训练过程开始。策略进一步迭代执行 MCTS 并生成解决方案，而值则影响树搜索过程。它们的更新在迭代中相互补充。

**o1复制之旅项目[110]**。该项目并未在两个阶段中全面考虑改进的实施，而是通过专注于全面的训练策略，旨在复制OpenAI的o1模型的推理能力。它强调了一种结构化的训练图，结合了试错、反思和回溯，以构建深层次的因果推理。该项目的核心是数据生成，设计了高质量的训练示例，以模拟复杂的推理路径。通过使用旅程学习法，o1复制之旅将模型暴露于各种逻辑序列和修正中，鼓励在训练阶段进行探索和适应性调整。然而，o1复制之旅在推理阶段较为简单，缺乏先进的训练后技术，这限制了其在实时推理中的适应性。这种对训练而非推理的重视，突显了其与具有动态推理优化模型相比的基础性方法。

**LLaMA-Berry[185]**项目专注于在推理阶段优化推理能力，利用LLaMA-3.1-8B架构实现更复杂的实时推理调整。该项目采用了一种独特的成对优化方法，将蒙特卡洛树搜索与自我优化（SR-MCTS）相结合，使模型能够在推理过程中动态探索和优化解决方案路径。这种配置赋予了LLaMA-Berry高度的适应性，使其能够高效且灵活地处理复杂的开放式推理任务。该框架的一个关键组成部分是成对偏好奖励模型（PPRM），它通过成对评估解决方案路径，确保优先选择高质量的推理路径。LLaMA-Berry的增强型博尔达计数法（EBC）进一步整合这些偏好排名，以指导模型的决策，从而提升其在推理阶段的精细度。这种强大的架构使LLaMA-Berry成为专注于推理强化的领先范例，与O1 Replication Journey以训练为中心的方法形成鲜明对比。

这四个开源框架不仅展示了强化推理的不同实现策略，还在提升对OpenAI o1模型的理解方面发挥了重要作用。它们共同扩展了开源社区可用的技术范围，推动了开发复杂、透明且适应性强的推理模型的集体目标，使公开可访问的系统具备专有级别的能力。

### Other Test-time Enhancing Techniques

除了PRM引导的搜索外，还有许多其他技术被设计出来，以通过更多的测试时计算来增强LLM的推理能力。这些技术在不修改模型本身的情况下，动态地优化推理结果。如图4所示，诸如语言强化搜索、基于记忆的强化和代理系统搜索等方法表明，仅使用现成的LLM就可以实现显著的推理改进。表3总结了探索这些方法的代表性工作。虽然这些方法没有利用PRM，但它们为未来研究探索混合模型以进一步提升推理能力提供了基础。

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=MjZjYmQ0NTUzMTg0N2ZlMjNhZGE1NGYzYzMzNzljZTJfS1JYOUpBdDVLYmZYNVhPV2xreVE1VnVGYzRMQXBDMjlfVG9rZW46SjdPNmI3dVRFb3ZTYkx4c1lMU2NTMnpwbm9iXzE3NDg3NTkyMjk6MTc0ODc2MjgyOV9WNA)

![](https://dppemvhuzp.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmE5YTNkNGJjMGJiOTA1ODJiNTFhYWUxN2ZjNzkxYzdfU2tXRFVzUXI3dVk0azVTaFNDWEMwRUZIdzUyeG5lMWdfVG9rZW46R1ZwZmJOUjVMb25udG94dXhzdWN3UmMxblpmXzE3NDg3NTkyMjk6MTc0ODc2MjgyOV9WNA)

#### Verbal Reinforcement Search

**Verbal Reinforcement Search (VRS)** 利用预训练大语言模型（LLMs）的推理和语义能力来探索和优化解决方案空间。与传统的强化学习或需要大量训练的方法不同，VRS 完全通过测试时的推理进行操作，利用迭代反馈循环来优化解决方案，而无需额外的训练。通过利用 LLMs 中编码的语义知识及其遵循复杂指令的能力，VRS 提供了一种多功能的方法，用于导航多样化的问题空间。这一推理驱动的框架适用于个体代理、多代理系统以及具身代理，支持广泛的任务，包括程序优化、协作决策以及现实环境中的交互。本节通过这三个关键方面对 VRS 进行分析，深入探讨每个类别中的方法论和独特见解。

在**个体代理设置**中，VRS（虚拟推理系统）依赖于迭代推理和反馈机制来在结构化问题空间中优化解决方案。这种方法非常适合诸如数学优化、符号推理和假设驱动的发现等任务，在这些任务中，系统化的优化显著提高了问题解决的效果。关于数学发现的研究展示了VRS如何将问题解决过程重塑为一个动态的迭代循环。例如，对组合问题的研究，包括帽集问题和在线装箱问题，强调了程序化解决方案如何通过反馈驱动的评估不断演化[115]。同样，符号回归研究将方程视为动态构造，通过迭代生成、评估和优化数学表达式[130]。这些方法展示了VRS如何在受限空间中导航，在效率和准确性上超越了传统的优化技术。在科学发现中，VRS展示了其在将推理与经验数据和模拟相结合方面的实用性。研究人员通过综合多种数据源开发了用于生物医学假设优化的系统。例如，在肿瘤学中的应用使用迭代合成来解决多尺度数据的复杂性[162]。在物理科学中，VRS通过模拟反馈来优化假设，推动了分子设计和物理定律发现等领域的发展[88]。这些发现强调了VRS在将抽象推理与现实世界验证相结合中的作用，支持那些既数据密集又假设驱动的任务。启发式优化中的反思过程进一步展示了VRS的灵活性。例如，研究人员探索了迭代生成和评估解决组合问题的策略[174]。这种方法专注于创建适应性强的超启发式算法，通过反馈循环不断优化解决方案，从而在不同领域中有效泛化。总体而言，VRS通过迭代推理和反馈将抽象问题解决与现实应用联系起来，以精确和适应性的方式应对数学、科学和优化中的挑战。

在**多智能体系统**中，VRS（虚拟推理系统）通过自然语言通信促进了基于LLM（大语言模型）的智能体之间的协作。这些系统利用共享推理和迭代优化来处理复杂的解决方案空间，使智能体能够交换见解并实现共同目标。在异构信息网络（HINs）中的元结构发现展示了VRS在多智能体环境中的应用。最近的研究将LLM推理与进化优化相结合，以优化元结构，增强其可解释性和预测准确性[20]。同样，在社会经济预测中，多智能体系统集成了知识图谱和元路径推理，以提取跨任务的见解，应用于人口估计和经济活动预测等领域。这种方法促进了LLM智能体之间的协作，并提高了多任务环境中的性能[199]。因果发现也受益于由VRS支持的多智能体框架。例如，使用LLM作为推理智能体的系统通过协作辩论并提出因果关系。通过结合统计方法和自然语言交互，这些框架生成了准确的因果图，同时解决了因果关系中的模糊性[69]。在金融决策中，VRS增强了层次化协作。FINCON框架采用经理-分析师系统，通过概念性语言强化来优化金融策略。通过减少冗余通信并改进策略优化，FINCON展示了VRS在优化金融决策过程中的实用性[176]。通过迭代优化和共享推理，VRS支持多智能体系统处理复杂的任务，如元结构优化、社会经济预测和金融决策。

在**具身代理（embodied agent）环**境中，虚拟推理系统（VRS）通过将推理与物理交互相结合，用于解决现实世界中的任务，支持诸如实验室环境中的实验规划与执行等活动。这些系统将VRS扩展到动态环境中，将语义推理与实际实验相结合。例如，自主化学研究已经展示了使用LLM（大型语言模型）驱动的系统独立设计、执行和优化实验的能力[13]。这些代理集成了诸如机器人液体处理器、光谱仪设备和基于网络的研究模块等工具，以执行反应优化和化合物合成等任务。一个应用案例涉及优化钯催化的交叉偶联反应，其中系统使用自然语言提示来确定条件、计算化学计量并自主执行实验。当遇到错误（如错误的模块调用）时，系统通过参考文档并迭代任务来修订其方法。这种迭代过程展示了VRS如何在实验工作流程中支持适应性和精确性。通过结合推理和实时反馈，具身代理展示了VRS在动态环境中优化复杂过程的能力。这些系统减少了人为干预，同时加速了科学发现，使其成为现实世界实验和创新的宝贵工具。

总体而言，以往的研究展示了VRS在个体代理、多代理系统和具身代理中的适应性和有效性。通过利用LLM的语义推理和迭代反馈能力，VRS能够处理广泛的任务，而无需额外的训练。从数学和科学背景中的结构化优化，到多代理框架中的协作探索，再到现实应用中的动态实验，VRS提供了一种统一的问题解决方法。VRS作为一个多功能框架，能够解决计算和物理领域中的复杂挑战，同时推动多个领域的进步。

#### Memory-based Reinforcement

当应用于开放式任务，如创意写作、复杂的逻辑推理和开放世界游戏时，解决方案空间往往会急剧扩展，常常变得无边界或定义不清。这些任务通常需要与环境进行持续互动以获取相关信息，这使得简单的解决方案空间搜索变得低效。为了解决这些挑战，一些研究为LLM（大型语言模型）代理引入了外部记忆模块。该模块存储了诸如观察结果、过去试验中的成功和失败行动等信息。代理通过迭代探索环境，利用记忆作为口头强化学习的基础。通过这一过程，他们总结经验，提取解决方案空间的可解释性高层次见解，并在后续试验中优化其行动，从而提高推理性能。这些研究不仅关注探索外部解决方案空间，还强调LLM代理从记忆中发展对解决方案空间理解的内在能力。随着代理通过环境探索积累记忆，他们的能力逐渐得到强化，并推广到未见过的任务中。具体来说，我们将这一领域的研究分为以下三类。

**体验学习**。这类方法鼓励LLM（大型语言模型）代理简单地模仿记忆中存储的有利经验，同时避免不利的经验。REMEMBERER [184] 引入了一种半参数的RL-LLM代理，该代理记录过去的观察-行动对，并使用传统的离策略Q学习算法动态维护和更新每个观察-行动对的Q值（预期未来奖励）。当面临新任务时，代理从记忆中检索具有最高和最低Q值的相关行动，并将这些作为鼓励和劝阻的示例纳入提示中。**记忆共享** [39] 利用多代理强化学习的概念来提高学习效率。多个代理在共享环境中并发执行任务，并将高质量的提示-答案对贡献到集体记忆池中。每个代理可以从该池中检索最相关的示例，以促进少样本学习。类似地，**体验共学习** [108] 采用多代理框架，其中教师和助理代理在多步代码生成过程中交替提供指令和解决方案。这种动态交换有助于提取捷径，以减少冗余并防止重复错误。当遇到新任务时，这些代理交替检索相关记忆，以提高上下文学习的效果。

**反思学习。**尽管使用记忆作为少样本示例是直接有效的，但这种方法并未充分利用大型语言模型（LLMs）的语义理解能力。一些研究认为，LLM代理应直接反思存储在记忆中的成功与失败，明确总结潜在原因，并将这些见解作为指导原则。Reflexion [129] 是这一领域的开创性工作，它基于任务反馈信号从语义上反思成功或失败的原因。它将反思文本和过去的轨迹整合到提示中，以增强后续试验中的决策能力。ExpeL [190] 结合了模仿和反思，通过从记忆中检索最相关的成功经验，总结成功轨迹的模式，并从成功-失败对的比较中识别见解。RAHL [138] 受分层强化学习的启发，将记忆组织为目标模块和子任务模块，从而在不同层次上实现反思和经验总结。对于新任务，它会检索相关经验，分别制定高层目标和低层子任务。

**概念学习**。显式反思显著增强了大型语言模型（LLMs）的推理能力。基于此，一些研究旨在使LLM代理能够发展出超越特定任务的“概念”，从而促进对环境和任务的更广泛理解。这种泛化帮助代理从记忆中内化认知能力，并随着记忆的增长不断进化。例如，Agent-Pro [188] 使代理能够在基于卡牌的游戏中建立关于自身和环境的信念。它不再反思个别行动，而是评估这些信念的合理性和一致性，迭代地优化策略。同样，Richelieu [44] 在军事策略游戏中赋予代理对环境理解的能力。它从记忆中检索最相关的状态来制定计划并评估可行性。通过自我对弈，它自主收集经验，扮演所有玩家的角色以提升其知识。Self-Evolving GPT [40] 受人类记忆机制的启发，为LLMs设计了一个基于记忆的自主学习框架。它通过分类任务来确定相关的记忆检索，并识别存储记忆与当前任务之间的差异，以提取共享的通用经验。此外，它还生成未见过的任务进行练习，基于记忆检索结果巩固其知识。

  

#### Agentic System Search

代理系统的设计在利用LLMs（大语言模型）进行许多下游任务中起着至关重要的作用。一种重要的测试时增强技术是利用LLMs来搜索代理系统。这一领域的研究可以分为三个搜索层次：**提示层、模块层和代理层**。需要注意的是，这种方法并不旨在直接搜索解决方案空间，而是利用经验数据来优化代理系统本身，这与元学习问题类似。我们将该领域的相关工作总结如下。

  

**提示层级。** “验证和修正”的过程通过迭代整合有用的反馈经验来改进提示。验证信号可以来自外部反馈 [43]、LLM 的自我评估 [90] 以及其他来源。另一方面，提示本身也值得搜索和优化。自动化的提示工程，如进化提示优化 [38] 和元提示迭代 [169]，可以比手动提示取得更好的结果，但也会引入更多的令牌消耗。

  

模块级别。Agentsquare [125] 提出使用LLM来搜索代理系统的模块化设计，其中模块本质上是具有特定功能的提示块，如规划、推理、工具使用和记忆。这些代理模块的基本单元具有标准的输入输出接口，使它们能够很好地相互协作。模块级别搜索的优势在于，它允许新代理通过模块的重组轻松重用经典的代理设计，如CoT和ToT。此外，Aflow[186]通过代码表示的边连接LLM的不同调用节点。除了搜索方法外，还需要评估搜索到的代理的性能。用于评估代理性能的函数也可以由LLM驱动，以提高搜索效率，同时紧密匹配其实际性能。

  

**代理层面。**ADAS提出利用LLM（大语言模型）来搜索整个以Python代码定义的代理系统空间[53]。此外，多代理系统在共享环境中做出决策并实现目标。在多代理层面的搜索中，关键方面包括代理创建、环境感知、行动、交互和系统演化。多代理系统的搜索在长故事生成等下游任务中取得了良好的效果[54]。目前正在探索多代理系统的统一搜索和优化机制。GPTSwarm[200]通过图优化增强了代理的协作能力。

  

代理系统搜索为代理提供了自我改进的能力，使其能够在不改变LLM结构的情况下优化自身，从而增强其推理能力。上述三个层面的搜索空间都非常庞大。这三个搜索层面面临的共同挑战是提高搜索效率、降低搜索成本，并在确保搜索合理性的同时实现自动化。

#### Summary

本节中回顾的测试时增强技术目前尚未被整合到大型推理模型的实现中。然而，它们具有巨大的潜力，通过更全面的测试时“思考”进一步提升大型语言模型（LLMs）的推理能力，帮助LLMs在解决方案空间中战略性地推理，利用过去的经验并动态优化代理工作流程。因此，训练LLMs掌握这些测试时技术代表了一个有前景的未来研究方向，并有可能将LLMs从“推理者”提升为完全功能的“代理”。

### Evaluation Benchmarks

设计一个稳健的基准测试对于记录大型语言模型（LLM）能力的提升至关重要。它在选择有前景的研究方向以进一步推动发展方面也起着关键作用。在本节中，我们系统性地回顾了用于LLM推理的流行基准测试，这些基准测试在图5的分类中进行了总结。接下来，我们将对这些基准测试进行如下讨论。

#### Math Problems

数学推理已成为评估大型语言模型（LLM）推理能力的重要测试平台。数学推理基准的领域从基础算术延伸到大学高级数学，提供了系统化的方法来评估不同方面的数学理解和问题解决能力。

  

在数学应用题（MWP）领域，基准从基本的算术运算逐步发展到更复杂的问题解决场景。在基础层面，像MATH-401 [177]这样的数据集通过401个精心构建的表达式来评估纯算术能力，而MultiArith [116]和AddSub [51]则评估将简单应用题转化为数学运算（如加法或减法）的能力。在小学和中学阶段，综合数据集如GSM8K [24]和MATH [50]提出了更复杂的多步推理挑战，其中GSM8K提供了8.5K个小学水平的问题，MATH则提供了12.5K个涵盖不同数学领域且难度逐级递增的问题。

  

对高级数学能力的评估主要通过竞赛和专门的测试数据集进行。像CHAMP [92]和ARB [5]这样的集合提供了竞赛级别的问题，这些问题需要复杂的解题策略，而MATHQA [5]则包含了来自GRE和GMAT考试的标准化测试题目。在最高级别，像FIMO [78]这样的数据集通过国际数学奥林匹克竞赛的问题来挑战模型，测试自动化数学推理的极限。

  

**几何推理**代表了一个需要空间理解和正式数学证明的独特类别。像Geometry3K [82]和GEOQA [19]这样的数据集提供了专门的几何问题，而UniGEO [18]则提供了一个统一的框架，专注于计算和证明的几何推理任务。这些基准在评估模型连接视觉和数学推理的能力方面特别有价值。

  

定理证明和形式数学领域已经发展出严格的评估框架。MINIF2F [193] 和 LeanDojo [170] 专注于与Lean定理相关的形式数学证明，而 THEOREMQA-MATH [23] 则考察对数学定理的理解。专门的数据库如 TRIGO [166] 和 PISA [57] 针对数学推理的特定领域，如三角学和形式证明系统。

  

最后，跨模态数学推理已成为一个关键领域，反映了现实世界中数学问题呈现的多样性。MATHVISTA [81] 和 CHARTQA [93] 通过图表和图形评估视觉数学推理，而 TABMWP [83] 和 MultiHiertt [192] 则评估处理表格和文本数据的能力。SciBench [151] 在纯数学和科学应用之间架起桥梁，测试在更广泛的科学背景下的数学推理能力。

#### Logical Problems

  

在数学推理能力的基础上，系统性逻辑推理能力是评估大语言模型（LLM）认知能力的另一项基本标准。数学推理侧重于定量操作和形式化证明，而逻辑推理则涵盖了更广泛的能力，包括在不同情境下得出有效结论、识别模式以及生成合理解释。根据Luo等人[86]的研究，逻辑推理可分为三种主要类型：演绎推理、归纳推理和溯因推理。每种类型都代表了一种独特的认知过程，这些过程在保持认知评估相互关联的同时，对于全面的逻辑分析至关重要。

  

  

**演绎推理**，也称为基于前提的推理，涉及从一般原则中推导出具体的结论，且具有绝对的确定性。例如，给定一组关于实体之间关系的规则，模型必须确定哪些具体关系必然成立。ProofWriter [140] 是这一类别的典型代表，它要求模型从给定的前提中构建明确的逻辑推导。其他基准测试，如FOLIO [46] 和 PrOntoQA [119]，评估了自然语境中的一阶逻辑推理能力，而WaNLI [77] 则通过107,885个例子引入了日益复杂的评估标准。

  

**归纳推理**强调从具体观察中识别模式并将其推广到更广泛的原则 [47]。这涉及识别潜在的规律性并将其扩展到新情境中，处理的是概率而非确定性。BigBench [136] 包含众多专门组件，用于检验高级模式推理能力。此外，CLUTTR [132] 基准系列通过不同复杂度的关系模式评估了这一能力。

  

**溯因推理**，也称为解释性推理，指的是为一系列观察或事实形成最可能解释的过程，尽管结论并不保证是确定的 [34]。这种推理通过生成合理的解释来测试模型如何处理信息不完整的场景。NLI [99] 基准通过叙事完成任务实现了这一点，模型必须为给定情境选择最可能的解释。AbductionRule [175] 系列提供了跨不同领域的结构化评估框架，并针对与动物和人物相关的推理场景提供了特定变体。ARCT [100] 专门考察了选择和合理解释以及论证理解的能力。

#### Commonsense Problems

  

常识推理在自然语言处理（NLP）领域仍然是一个重大挑战，其目标是评估大语言模型（LLM）理解和应用日常常识知识的能力。目前存在多种针对不同维度的常识推理任务的基准测试。例如，CommonsenseQA [141] 要求模型基于常识知识库回答推理问题。

  

SocialIQA [118] 则专注于社交互动中的常识推理，特别是社交场景中的因果推理。相比之下，SWAG [178] 和 HellaSwag [179] 等数据集引入了对抗性文本推理任务，要求模型根据上下文线索预测事件的最可能延续，从而增加了任务的复杂性。在物理常识推理方面，PIQA [12] 和 PHYRE [10] 等基准测试主要评估模型对日常物理任务和交互式推理场景的理解能力。PIQA 主要采用问答任务，而 PHYRE 则强调交互式物理模拟。类似地，WinoGrande [117] 在 Winograd Schema Challenge 的基础上，引入了更大规模的数据集和更复杂的消歧任务，以测试语义理解和共指消解能力。

  

其他研究，如 OBQA [95] 和 CConS [63]，探索了模型在反常识情境下的表现，突显了当前模型在隐式推理和背景知识利用方面面临的挑战。最近，MMLU [49] 等综合性基准测试和 FactCC [66] 等批判性研究进一步分析了大语言模型的常识推理和事实推理能力。这些基准测试为语言模型的泛化能力提供了宝贵的视角，并成为评估和改进其在多样化常识推理任务中表现的重要工具。

#### Coding Problem

  

代码生成基准的发展在评估大型语言模型（LLMs）在编程任务中的推理能力方面发挥了重要作用。这些基准评估模型在不同领域中生成准确、高效和可靠代码的能力。例如，ODEX [155] 引入了一种执行驱动的开放域代码生成评估框架，强调通过运行生成的代码来验证其正确性和功能的重要性。

  

在现实场景中，SWE-bench [58] 专注于真实的GitHub问题，挑战模型解决实际软件工程问题的能力。在数据科学领域，DS-1000 [67] 提供了一个包含真实且可靠的数据科学代码生成任务的基准，使评估模型处理复杂数据操作和分析的能力成为可能。此外，APPS基准 [49] 通过评估模型在多样化编程问题上的表现来衡量其编程挑战能力，反映了在竞争性编程和技术面试中遇到的挑战。

  

MBPP [6] 专注于程序合成问题，评估模型根据给定规范生成正确且高效代码的能力，从而有助于理解LLMs在自动代码生成中的能力。HumanEval [21] 通过提供一组Python编程问题来评估经过代码训练的LLMs，每个问题都附带函数定义和相关文档，要求模型生成正确且功能完备的代码解决方案。

#### Agent Problems

  

基于智能体的基准测试的出现，彻底改变了我们在交互环境中评估大型语言模型（LLMs）作为独立智能体的能力。这些复杂的评估框架能够评估包括决策、推理和跨多种场景的环境交互在内的关键能力。

  

WebArena [197] 提供了一个实用的网络环境，用于构建和测试自主智能体，从而评估LLMs的网络导航和交互技能。同样，Mind2Web [28] 旨在开发能够跨多种网络任务操作的通才智能体，强调在动态在线环境中的适应性。

  

在电子商务场景中，WebShop [171] 引入了一个可扩展的现实世界网络交互平台，专注于能够执行在线购物等任务的基于语言的智能体，从而测试模型的实际应用能力。为了连接文本和具身环境，ALFWorld [131] 将基于文本的输入与交互学习场景对齐，促进评估模型在不同模态之间传递知识的能力。

  

AgentBench [79] 和 AgentGym [164] 等综合评估框架被开发出来，以系统性地评估作为智能体的LLMs。AgentBench 包含多种环境，用于评估推理和决策能力，而 AgentGym 则专注于在不同环境中进化的基于LLM的智能体，强调适应性和学习效率。此外，AgentBoard [87] 提供了一个分析平台，用于评估多轮LLM智能体，提供其在长期交互中的表现洞察，并突出在持续推理任务中的改进领域。

### Discussion

#### Inspirations from the recent advances

  

**后训练阶段的缩放定律**。OpenAI o1系列的启发带来了对预训练-后训练-推理阶段的新理解。特别是在后训练阶段，引入了自我对弈强化学习和高质量思维链标注数据的过程奖励学习。此外，它还延伸到后训练阶段的缩放定律，这为训练阶段缩放定律进一步发展的困难提供了启示。众所周知，预训练和训练阶段的缩放定律导致了流行大语言模型（LLMs）的成功，这得益于大量的训练数据和计算资源的投入。然而，目前已经达到了瓶颈，因此，后训练阶段的缩放定律可能是大语言模型下一阶段发展的驱动力。此外，即使推理能力尚未得到加强，精心设计的工作流程也显示出LLM驱动代理[163]的巨大潜力。因此，关于LLM代理在资源消耗和性能方面是否也存在类似的缩放定律，仍是一个开放性问题，这可能是进一步提升LLM在实际应用中潜力的关键。最后，当前表现出的测试时缩放定律与模型遵循指令的能力之间可能存在某种关系；也就是说，模型必须具备足够强大的指令遵循能力，才能展示出测试时的缩放定律。例如，语言强化搜索技术的成功要求LLMs具备基本的指令遵循能力。因此，如果LLMs无法准确遵循指令，复杂的后训练技术可能无法正常工作。

  

  

**通过搜索生成高质量数据**。OpenAI核心技术人员披露的o1系列技术理念，以及目前试图复现OpenAI o1的开源作品，都将高质量数据（包括CoT数据）的生成视为关键点，尽管采用了不同的方法，如蒙特卡洛树搜索、LLM生成等。也就是说，大型推理模型的发展已经进入了一个阶段，其中高质量的过程奖励数据比一般的预训练数据规模更为重要。同样，正如上文所讨论的，这可能会启发我们在LLM代理中也参考这些相关方法，首先进行高质量数据的生成，然后增强慢速推理的学习以及能力的获取。

#### Slow-thinking and reasoning

  

即使OpenAI o1系列在工程层面的突破尚不明确，从理论和技术的角度来看，其突破目前似乎主要集中在对慢思维数据的后训练学习上。此外，尽管“系统1与系统2”的人类认知科学被多次提及，但基于大模型实现这一理念的思路仍在不断更新，主要仍停留在借鉴慢思维概念的阶段。也就是说，人类大脑的“系统1与系统2”机制虽然指导了大语言模型（LLMs）的设计，但这种指导仍然非常有限。换言之，对人类大脑的模仿仅停留在系统层面的设计，而非非常详细的技术实现。人类慢思维的复杂机制及其优势仍显示出巨大的潜力，可以支持LLMs的下一级推理能力。为了实现这一点，慢思维的领域知识应被用于相关设计中，例如推理数据生成、奖励函数、学习过程等。

  

迄今为止，关于LLMs慢思维的理论分析尚未出现真正具有代表性和重要性的工作。生成式人工智能如此神秘，以至于理解LLMs也需要一些技巧或特殊技术，例如用于理解LLM幻觉的新指标[37]。要理解慢推理能力，我们可能还需要深入理论分析。以OpenAI o1 Preview和OpenAI o1 Mini这两个不同版本为例，主要区别在于CoT推理阶段的成本和思维深度，但它们在文本生成、代码生成和数学问题解决等任务中表现出显著差异。LLMs所展现的推理特性也启发我们设计任务自适应的使用和应用。具体而言，将推理机制与不同任务中的表现联系起来，可能会支持更有趣的洞察。

#### Downstream applications and open problems

  

  

正如本文所述，推理增强技术的发展速度非常迅速。推理能力不仅限于这些流行的基准任务，还体现在下游应用中更为广泛的任务上。例如，FunSearch 工作 [115] 展示了在难以提供解决方案但验证速度快的任务中的通用能力。在多个领域，如城市规划、物流调度等，可能存在一系列具有类似特征的任务。一个有趣的问题是，当前研究中是否存在许多互补问题，这些问题难以验证，但推理过程却相对简单。或许可以通过结合大型语言模型（LLMs）和外部评估工具来进一步验证某些答案的质量，或者我们可以使用这些经过评分评估的答案来训练奖励模型。

### Conclusion

  

近年来，大型语言模型（LLMs）的演进显著提升了其类人推理能力。通过引入“思维”作为中间步骤的概念、利用强化学习技术进行训练阶段的扩展，以及使用搜索算法进行测试阶段的扩展，这些创新为大型推理模型奠定了基础。这些模型能够处理日益复杂的认知任务，OpenAI的o1系列便是典型代表。该领域的持续进展有望重塑我们对语言的理解，并推动人工智能在解决现实问题中的应用。
